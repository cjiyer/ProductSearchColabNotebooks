{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Import necessary libraries\n",
    "import concurrent.futures\n",
    "import math\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Constants\n",
    "API_ENDPOINT = \"http://localhost:30200/search\"\n",
    "QUERY_PARAM = \"query\"  # Column name from the dataset containing the queries\n",
    "ALGO = \"COMMERCE_AI_SEARCH.latest\"\n",
    "INDEX = \"PRODUCT_ESCI\"\n",
    "TOP_K = 5  # Number of top results to fetch from the API (k for NDCG@k and MRR@k)\n",
    "MAX_WORKERS = 4  # Number of parallel workers for REST queries\n",
    "NUM_RETRIES = 5\n",
    "\n",
    "# Metrics calculation functions (These remain unchanged)\n",
    "\n",
    "def calculate_precision_recall_f1_score(predicted, true_labels, k):\n",
    "    # Same as provided code (unchanged)\n",
    "    predicted_top_k = set(predicted[:k])\n",
    "    true_labels_set = set(true_labels)\n",
    "\n",
    "    precision = len(predicted_top_k & true_labels_set) / len(predicted_top_k) if len(predicted_top_k) > 0 else 0\n",
    "    recall = len(predicted_top_k & true_labels_set) / len(true_labels_set) if len(true_labels_set) > 0 else 0\n",
    "\n",
    "    if precision + recall == 0:\n",
    "        f1_score = 0\n",
    "    else:\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1_score\": f1_score}\n",
    "\n",
    "def calculate_graded_ndcg_at_k(predicted, ground_truth, labels, graded_ground_truth, true_labels, k):\n",
    "    # Same as provided code (unchanged)\n",
    "    dcg = 0.0\n",
    "    idcg = sum([true_labels[i] / math.log2(i + 2) for i in range(min(k, len(graded_ground_truth)))])\n",
    "\n",
    "    for i, result in enumerate(predicted[:k]):\n",
    "        if result in ground_truth and i < len(labels):\n",
    "            dcg += labels[i] / math.log2(i + 2)\n",
    "    ndcg = dcg / idcg if idcg > 0 else 0\n",
    "    return ndcg\n",
    "\n",
    "def calculate_ndcg_at_k(predicted, ground_truth, k):\n",
    "    \"\"\"\n",
    "    Calculate Normalized Discounted Cumulative Gain (NDCG) @ k.\n",
    "\n",
    "    Args:\n",
    "        predicted (list): List of predicted labels.\n",
    "        ground_truth (list): List of ground truth labels.\n",
    "        k (int): Number of top results (k).\n",
    "\n",
    "    Returns:\n",
    "        float: NDCG@k.\n",
    "    \"\"\"\n",
    "    dcg = 0.0\n",
    "    idcg = sum([1 / math.log2(i + 2) for i in range(min(k, len(ground_truth)))])\n",
    "\n",
    "    for i, result in enumerate(predicted[:k]):\n",
    "        if result in ground_truth:\n",
    "            dcg += 1 / math.log2(i + 2)\n",
    "\n",
    "    ndcg = dcg / idcg if idcg > 0 else 0\n",
    "    return ndcg\n",
    "\n",
    "\n",
    "def calculate_map_at_k(predicted, ground_truth, k):\n",
    "    hits,ap=0,0.0\n",
    "    for i, result in enumerate(predicted[:k]):\n",
    "        if result in ground_truth:\n",
    "            hits+=1\n",
    "            ap += hits / (i + 1)\n",
    "    return ap/max(1,len(ground_truth)) if ground_truth else 0\n",
    "\n",
    "def calculate_mrr_at_k(predicted, ground_truth, k):\n",
    "    # Same as provided code (unchanged)\n",
    "    for i, result in enumerate(predicted[:k]):\n",
    "        if result in ground_truth:\n",
    "            return 1 / (i + 1)\n",
    "    return 0\n",
    "\n",
    "def sanitize_query(query):\n",
    "\n",
    "    special_chars = '#?\\'\"()*+,-/'\n",
    "    query = query.strip(special_chars)\n",
    "    return query.strip()\n",
    "\n",
    "# Updated fetch API results\n",
    "def fetch_results(query):\n",
    "    \"\"\"\n",
    "    Fetch product titles from API.\n",
    "    Args:\n",
    "        query (str): Search query to send to API.\n",
    "\n",
    "    Returns:\n",
    "        dict: Product titles and query result.\n",
    "    \"\"\"\n",
    "\n",
    "    params = {\n",
    "        \"query\": sanitize_query(query),\n",
    "        \"algo\": ALGO,\n",
    "        \"limit\": TOP_K,\n",
    "        \"noAggs\": \"true\",\n",
    "        \"searchIndex\": INDEX\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(API_ENDPOINT, params=params)\n",
    "        if response.status_code == 200:\n",
    "            response_json = response.json()\n",
    "            product_ids = [product[\"parent_id\"] for product in response_json.get(\"products\", [])]\n",
    "            return {\"query\": query, \"results\": product_ids}\n",
    "        else:\n",
    "            return {\"query\": query, \"results\": [], \"error\": f\"HTTP {response.status_code}\"}\n",
    "    except Exception as e:\n",
    "        return {\"query\": query, \"results\": [], \"error\": str(e)}\n",
    "\n",
    "# Parallelized version to fetch results concurrently for all queries\n",
    "def fetch_results_parallel(queries):\n",
    "    \"\"\"\n",
    "    Perform parallel fetching of product results for the given queries using ThreadPoolExecutor.\n",
    "\n",
    "    Args:\n",
    "        queries (list): List of queries to execute.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing query results and errors (if any).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    print(f\"Starting parallel execution with {MAX_WORKERS} workers...\")\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    completed_queries = 0\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        future_to_query = {executor.submit(fetch_results, query): query for query in queries}\n",
    "        for future in concurrent.futures.as_completed(future_to_query):\n",
    "            try:\n",
    "                result = future.result()  # Get the result of the completed future object\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                query = future_to_query[future]\n",
    "                print(f\"Error during query execution '{query}': {str(e)}\")\n",
    "            completed_queries += 1  # Initialize a counter for the completed queries\n",
    "            if (completed_queries % 200) == 0:\n",
    "                elapsed_time = time.perf_counter() - start_time\n",
    "                print(f\"200 Queries processed with total queries processed size : {completed_queries} with processed time taken : {elapsed_time:.2f} seconds\")\n",
    "\n",
    "    print(f\"Completed parallel execution for {len(queries)} queries.\")\n",
    "    return results\n",
    "\n",
    "# Main evaluation loop with parallelized REST queries\n",
    "def evaluate_queries_parallel(df_train_small_us):\n",
    "    \"\"\"\n",
    "    Evaluate the queries from a dataset using API results and calculate metrics parallelly.\n",
    "\n",
    "    Args:\n",
    "        df_train_small_us (pd.DataFrame): DataFrame containing queries and ground truth.\n",
    "\n",
    "    Returns:\n",
    "         pd.DataFrame: DataFrame containing evaluation metrics for all queries.\n",
    "    \"\"\"\n",
    "    # Prepare results\n",
    "    results = []\n",
    "\n",
    "    # Extract all unique queries\n",
    "    queries_to_execute = df_train_small_us[QUERY_PARAM].unique()\n",
    "    # Process each query result using the ground truth\n",
    "    empty_result_cnt = 0\n",
    "    error_cnt = 0\n",
    "    start_time = time.perf_counter()\n",
    "    for i in range(NUM_RETRIES):\n",
    "        print(f\"Total number of queries to execute {len(queries_to_execute)} at iteration {i}.\")\n",
    "        if len(queries_to_execute) == 0:\n",
    "            break\n",
    "        # Perform parallel query execution\n",
    "        parallel_results = fetch_results_parallel(queries_to_execute)\n",
    "\n",
    "        queries_to_execute = []\n",
    "        for query_result in parallel_results:\n",
    "            query = query_result.get(\"query\")\n",
    "            predicted_product_ids = query_result.get(\"results\", [])\n",
    "            error = query_result.get(\"error\")\n",
    "\n",
    "            df_query_train_small_us = df_train_small_us[\n",
    "                df_train_small_us[QUERY_PARAM] == query\n",
    "            ].sort_values(by=['example_id', 'query_id', 'esci_graded_label'], ascending=[True, True, False])\n",
    "            query_id = df_query_train_small_us['query_id'].unique()[0]\n",
    "            graded_df_query_train_small_us = df_query_train_small_us.sort_values(by=['query_id', 'esci_graded_label', 'example_id'], ascending=[True, False, True])\n",
    "\n",
    "            ground_truth_product_ids = df_query_train_small_us['product_id'].tolist()\n",
    "            ground_truth_labels = df_query_train_small_us['esci_graded_label'].tolist()\n",
    "            graded_ground_truth_product_ids = graded_df_query_train_small_us['product_id'].tolist()\n",
    "            graded_ground_truth_labels = graded_df_query_train_small_us['esci_graded_label'].tolist()\n",
    "\n",
    "            if error:\n",
    "                # print(f\"API error for query '{query}': {error}\")\n",
    "                if i == (NUM_RETRIES-1):\n",
    "                    results.append({\n",
    "                        \"query_id\": query_id,\n",
    "                        \"query\": query,\n",
    "                        \"ground_truth\": ground_truth_product_ids,\n",
    "                        \"predicted\": error,\n",
    "                        \"precision\": -1.0,\n",
    "                        \"recall\": -1.0,\n",
    "                        \"f1_score\": -1.0,\n",
    "                        \"ndcg@k\": -1.0,\n",
    "                        \"graded_ndcg@k\": -1.0,\n",
    "                        \"map@k\": -1.0,\n",
    "                        \"mrr@k\": -1.0\n",
    "                    })\n",
    "                    error_cnt += 1\n",
    "                queries_to_execute.append(query)\n",
    "                continue\n",
    "            else:\n",
    "                if not predicted_product_ids or len(predicted_product_ids) == 0:\n",
    "                    empty_result_cnt += 1\n",
    "\n",
    "            # Calculate metrics\n",
    "            metrics = calculate_precision_recall_f1_score(predicted_product_ids, ground_truth_product_ids, TOP_K)\n",
    "            ndcg_at_k = calculate_ndcg_at_k(predicted_product_ids, ground_truth_product_ids, TOP_K)\n",
    "            graded_ndcg_at_k = calculate_graded_ndcg_at_k(predicted_product_ids, ground_truth_product_ids, ground_truth_labels, graded_ground_truth_product_ids, graded_ground_truth_labels, TOP_K)\n",
    "            map_at_k = calculate_map_at_k(predicted_product_ids, ground_truth_product_ids, TOP_K)\n",
    "            mrr_at_k = calculate_mrr_at_k(predicted_product_ids, ground_truth_product_ids, TOP_K)\n",
    "\n",
    "            # Add to results\n",
    "            results.append({\n",
    "                \"query_id\": query_id,\n",
    "                \"query\": query,\n",
    "                \"ground_truth\": ground_truth_product_ids,\n",
    "                \"predicted\": predicted_product_ids,\n",
    "                **metrics,\n",
    "                \"ndcg@k\": ndcg_at_k,\n",
    "                \"graded_ndcg@k\": graded_ndcg_at_k,\n",
    "                \"map@k\": map_at_k,\n",
    "                \"mrr@k\": mrr_at_k\n",
    "            })\n",
    "    print(f\"Total Empty Results: {empty_result_cnt} and total Errors: {error_cnt}\")\n",
    "    elapsed_time = time.perf_counter() - start_time\n",
    "    print(f\"Parallel query execution completed in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    # Return as DataFrame\n",
    "    result_df = pd.DataFrame(results)\n",
    "    result_df.sort_values(by=['query_id'], ascending=[True], inplace=True)\n",
    "    return result_df\n",
    "\n",
    "# Data Loading\n",
    "df_examples = pd.read_parquet('/Users/chiyer/misc/projects/honeyevaluation/datasets/golden/esci_shopping_queries/shopping_queries_dataset_examples.parquet')\n",
    "df_products = pd.read_parquet('/Users/chiyer/misc/projects/honeyevaluation/datasets/golden/esci_shopping_queries/shopping_queries_dataset_products.parquet')\n",
    "print(df_products.shape)\n",
    "df_examples_products = pd.merge(\n",
    "    df_examples,\n",
    "    df_products,\n",
    "    how='inner',\n",
    "    left_on=['product_locale','product_id'],\n",
    "    right_on=['product_locale', 'product_id']\n",
    ")\n",
    "df_examples_products_train_small_us = df_examples_products[(df_examples_products[\"split\"] == 'train') & (df_examples_products[\"small_version\"] == 1) & (df_examples_products[\"product_locale\"] == 'us')]\n",
    "\n",
    "ESCI_GRADED_MAP = {\"E\":3,\"S\":2,\"C\":1,\"I\":0}\n",
    "df_examples_products_train_small_us['esci_graded_label'] = df_examples_products_train_small_us['esci_label'].map(ESCI_GRADED_MAP)\n",
    "df_examples_products_train_small_us = df_examples_products_train_small_us[[\"example_id\",\"query_id\", \"query\", \"product_id\", \"product_title\", \"esci_graded_label\"]].drop_duplicates()\n",
    "df_examples_products_train_small_us = df_examples_products_train_small_us[df_examples_products_train_small_us[\"esci_graded_label\"] > 0]  #Filter out Irrelevant Labels (esci_graded_label > 0)\n",
    "# df_examples_products_train_small_us.sort_values(by=['example_id', 'query_id', 'esci_graded_label'], ascending=[True, True, False], inplace=True)\n",
    "print(df_examples_products_train_small_us.shape)\n",
    "\n",
    "# Evaluate queries with parallel REST calls\n",
    "print(\"Running parallel evaluations on dataset...\")\n",
    "evaluation_results = evaluate_queries_parallel(df_examples_products_train_small_us)\n",
    "\n",
    "query_output_path = f\"/Users/chiyer/misc/projects/honeyevaluation/output/{ALGO}_query_metrics_TOP{TOP_K}_v0.tsv\"\n",
    "evaluation_results.to_csv(query_output_path, sep='\\t', index=False)\n",
    "print(f\"Evaluation results saved to {query_output_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "filtered_evaluation_results = evaluation_results[evaluation_results['predicted'] != \"HTTP 500\"]\n",
    "metric_columns = ['precision', 'recall', 'f1_score', 'ndcg@k', 'graded_ndcg@k', 'map@k', 'mrr@k']\n",
    "for col in metric_columns:\n",
    "    filtered_evaluation_results[col] = pd.to_numeric(filtered_evaluation_results[col], errors='coerce')\n",
    "\n",
    "# Calculate the average of the metric columns\n",
    "summary_metrics = filtered_evaluation_results[metric_columns].mean()\n",
    "summary_output_path = f\"/Users/chiyer/misc/projects/honeyevaluation/output/{ALGO}_summary_metrics_TOP{TOP_K}_v0.tsv\"\n",
    "summary_metrics.to_csv(summary_output_path, sep='\\t', index=False)\n",
    "print(f\"Evaluation results saved to {summary_output_path}\")"
   ],
   "id": "ae6dda171a3628a6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
