{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c065116e",
      "metadata": {
        "id": "c065116e"
      },
      "source": [
        "\n",
        "# ðŸ“Š Offline Search Evaluation on Amazon ESCI\n",
        "\n",
        "This notebook implements an **offline evaluation prototype** for e-commerce search using the **Amazon ESCI** dataset.\n",
        "- Dataset: [amazon-science/esci-data](https://github.com/amazon-science/esci-data)\n",
        "- Algorithms: BM25 (keyword-based) and SBERT+FAISS (semantic)\n",
        "- Metrics: nDCG@K, MAP@K, MRR, Precision/Recall@K\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install libomp-dev"
      ],
      "metadata": {
        "id": "DDZhsjjOOjt-"
      },
      "id": "DDZhsjjOOjt-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-gpu-cu12"
      ],
      "metadata": {
        "id": "X_I2f-ejO_JC"
      },
      "id": "X_I2f-ejO_JC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import Libraries\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from typing import List, Dict, Tuple\n",
        "import gc\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)"
      ],
      "metadata": {
        "id": "3c7WGQdPgVJA"
      },
      "id": "3c7WGQdPgVJA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "id": "aWbJ-RgVR3-z"
      },
      "id": "aWbJ-RgVR3-z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Define text cleaning function\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Clean text by removing HTML tags, emojis, and special characters.\n",
        "    Args:\n",
        "        text: Input raw text (string).\n",
        "    Returns:\n",
        "        Cleaned text.\n",
        "    \"\"\"\n",
        "    # Remove HTML tags\n",
        "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
        "\n",
        "    # Remove emojis and non-alphanumeric characters\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove emojis and non-ASCII characters\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s:]', '', text)  # Retain alphabetical, numerical, and colon characters\n",
        "\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "H9vxhcbmm_70"
      },
      "id": "H9vxhcbmm_70",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_examples = pd.read_parquet('gs://chanderiyer/datasets/esci_shopping/shopping_queries_dataset_examples.parquet')\n",
        "df_products = pd.read_parquet('gs://chanderiyer/datasets/esci_shopping/shopping_queries_dataset_products.parquet')\n",
        "\n",
        "df_products[\"product_description\"] = df_products[\"product_description\"].apply(lambda x: clean_text(x) if not pd.isnull(x) else \"\")\n",
        "df_products[\"product_bullet_point\"] = df_products[\"product_bullet_point\"].apply(lambda x: clean_text(x) if not pd.isnull(x) else \"\")\n",
        "\n",
        "# Combine title, bullet_point, description\n",
        "for c in [\"product_title\",\"product_description\",\"product_bullet_point\",\"product_brand\"]:\n",
        "    if c in df_products.columns:\n",
        "        df_products[c] = df_products[c].fillna(\"\").astype(str)\n",
        "\n",
        "df_products[\"product_full_description\"] = df_products[\"product_title\"] + \" \" + df_products[\"product_bullet_point\"] + \" \" + df_products[\"product_description\"]\n",
        "\n",
        "df_products[\"doc_text\"] = df_products[[\"product_full_description\",\"product_brand\"]].agg(\" \".join, axis=1)\n",
        "df_products = df_products[[\"product_id\",\"doc_text\", \"product_locale\"]].drop_duplicates()\n",
        "\n",
        "df_examples_products = pd.merge(\n",
        "    df_examples,\n",
        "    df_products,\n",
        "    how='inner',\n",
        "    left_on=['product_locale','product_id'],\n",
        "    right_on=['product_locale', 'product_id']\n",
        ")\n",
        "train_df = df_examples_products[(df_examples_products[\"small_version\"] == 1) & (df_examples_products[\"product_locale\"] == \"us\") & (df_examples_products[\"split\"] == \"train\")]\n",
        "train_df.head(n=30)"
      ],
      "metadata": {
        "id": "-Oa2aoC-hGPA"
      },
      "id": "-Oa2aoC-hGPA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Map ESCI Labels to Graded Relevance\n",
        "label_mapping = {\n",
        "    'E': 3,  # Exact\n",
        "    'S': 2,  # Substitute\n",
        "    'C': 1,  # Complement\n",
        "    'I': 0   # Irrelevant\n",
        "}\n",
        "\n",
        "train_df['relevance_score'] = train_df['esci_label'].map(label_mapping)"
      ],
      "metadata": {
        "id": "8b5maic3hGGT"
      },
      "id": "8b5maic3hGGT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Prepare Data for Embedding\n",
        "# Get unique queries and products\n",
        "unique_queries = train_df['query'].unique()\n",
        "unique_products = df_products[['product_id', 'doc_text']].drop_duplicates()\n",
        "\n",
        "print(f\"Number of unique queries: {len(unique_queries)}\")\n",
        "print(f\"Number of unique products: {len(unique_products)}\")"
      ],
      "metadata": {
        "id": "DkWxWu17hGBp"
      },
      "id": "DkWxWu17hGBp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Load Model on GPU\n",
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model = SentenceTransformer(model_name)\n",
        "model = model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "G0Oef2QJhF62"
      },
      "id": "G0Oef2QJhF62",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Generate Query Embeddings on GPU\n",
        "def encode_texts_gpu(texts: List[str], model: SentenceTransformer, batch_size: int = 256) -> np.ndarray:\n",
        "    \"\"\"Encode texts using GPU with batching and mixed precision\"\"\"\n",
        "    embeddings = []\n",
        "\n",
        "    with torch.amp.autocast('cuda'):\n",
        "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding\"):\n",
        "            batch = texts[i:i + batch_size]\n",
        "            batch_embeddings = model.encode(\n",
        "                batch,\n",
        "                convert_to_tensor=True,\n",
        "                show_progress_bar=False\n",
        "            )\n",
        "            embeddings.append(batch_embeddings.cpu().numpy())\n",
        "\n",
        "            # Clear GPU cache periodically\n",
        "            if i % (batch_size * 10) == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "# Generate embeddings\n",
        "print(\"Generating query embeddings...\")\n",
        "query_embeddings = encode_texts_gpu(unique_queries.tolist(), model)\n",
        "\n",
        "print(\"Generating product embeddings...\")\n",
        "product_titles = unique_products['doc_text'].tolist()\n",
        "product_embeddings = encode_texts_gpu(product_titles, model)\n",
        "\n",
        "# Create mappings\n",
        "query_to_idx = {q: i for i, q in enumerate(unique_queries)}\n",
        "product_to_idx = {pid: i for i, pid in enumerate(unique_products['product_id'].values)}"
      ],
      "metadata": {
        "id": "OmvB3VtghF4P"
      },
      "id": "OmvB3VtghF4P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Create GPU FAISS Index\n",
        "def create_gpu_faiss_index(embeddings: np.ndarray) -> faiss.Index:\n",
        "    \"\"\"Create GPU-optimized FAISS index\"\"\"\n",
        "    dimension = embeddings.shape[1]\n",
        "\n",
        "    # Create CPU index\n",
        "    cpu_index = faiss.IndexFlatIP(dimension)\n",
        "\n",
        "    # Move to GPU\n",
        "    res = faiss.StandardGpuResources()\n",
        "    gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index)\n",
        "\n",
        "    # Normalize embeddings for cosine similarity\n",
        "    faiss.normalize_L2(embeddings)\n",
        "\n",
        "    # Add embeddings to index\n",
        "    gpu_index.add(embeddings.astype(np.float32))\n",
        "\n",
        "    return gpu_index\n",
        "\n",
        "# Create GPU index\n",
        "print(\"Creating GPU FAISS index...\")\n",
        "gpu_index = create_gpu_faiss_index(product_embeddings)"
      ],
      "metadata": {
        "id": "7GbnkE1zhFzD"
      },
      "id": "7GbnkE1zhFzD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Perform Retrieval for Each Query\n",
        "def retrieve_top_k(query_embeddings: np.ndarray, index: faiss.Index, k: int = 100) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Retrieve top-k products for each query using GPU\"\"\"\n",
        "    # Normalize query embeddings\n",
        "    query_embeddings_normalized = query_embeddings.copy()\n",
        "    faiss.normalize_L2(query_embeddings_normalized)\n",
        "\n",
        "    # Search\n",
        "    similarities, indices = index.search(query_embeddings_normalized.astype(np.float32), k)\n",
        "\n",
        "    return similarities, indices\n",
        "\n",
        "# Retrieve top-100 products for each query\n",
        "print(\"Performing retrieval...\")\n",
        "k = 100\n",
        "similarities, indices = retrieve_top_k(query_embeddings, gpu_index, k)"
      ],
      "metadata": {
        "id": "Y4inmxiQhFuM"
      },
      "id": "Y4inmxiQhFuM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Compute Evaluation Metrics\n",
        "def compute_graded_metrics(train_df: pd.DataFrame, query_to_idx: Dict, product_to_idx: Dict,\n",
        "                          indices: np.ndarray, unique_queries: np.ndarray,\n",
        "                          unique_products: pd.DataFrame, k: int = 100) -> Dict:\n",
        "    \"\"\"Compute graded relevance metrics\"\"\"\n",
        "\n",
        "    metrics = {\n",
        "        'que'\n",
        "        'precision_at_k': [],\n",
        "        'recall_at_k': [],\n",
        "        'f1_at_k': [],\n",
        "        'map_scores': [],\n",
        "        'ndcg_scores': [],\n",
        "        'mrr_scores': []\n",
        "    }\n",
        "\n",
        "    for query_idx, query in enumerate(tqdm(unique_queries, desc=\"Computing metrics\")):\n",
        "        # Get ground truth for this query\n",
        "        query_data = train_df[train_df['query'] == query]\n",
        "\n",
        "        # Create ground truth relevance dictionary\n",
        "        ground_truth = {}\n",
        "        for _, row in query_data.iterrows():\n",
        "            if row['product_id'] in product_to_idx:\n",
        "                ground_truth[product_to_idx[row['product_id']]] = row['relevance_score']\n",
        "\n",
        "        # Get retrieved product indices\n",
        "        retrieved_indices = indices[query_idx][:k]\n",
        "\n",
        "        # Compute relevance scores for retrieved items\n",
        "        relevance_scores = []\n",
        "        for idx in retrieved_indices:\n",
        "            relevance_scores.append(ground_truth.get(idx, 0))\n",
        "\n",
        "        relevance_scores = np.array(relevance_scores)\n",
        "\n",
        "        # Precision@k (binary: relevant if score > 0)\n",
        "        num_relevant_retrieved = np.sum(relevance_scores > 0)\n",
        "        precision = num_relevant_retrieved / k if k > 0 else 0\n",
        "\n",
        "        # Recall@k\n",
        "        total_relevant = np.sum(np.array(list(ground_truth.values())) > 0)\n",
        "        recall = num_relevant_retrieved / total_relevant if total_relevant > 0 else 0\n",
        "\n",
        "        # F1@k\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "        # Average Precision (graded)\n",
        "        ap = 0\n",
        "        num_relevant_seen = 0\n",
        "        for i, score in enumerate(relevance_scores):\n",
        "            if score > 0:\n",
        "                num_relevant_seen += 1\n",
        "                ap += num_relevant_seen / (i + 1)\n",
        "        ap = ap / total_relevant if total_relevant > 0 else 0\n",
        "\n",
        "        # NDCG@k\n",
        "        dcg = np.sum(relevance_scores / np.log2(np.arange(1, k + 1) + 1))\n",
        "        ideal_scores = sorted([s for s in ground_truth.values() if s > 0], reverse=True)[:k]\n",
        "        idcg = np.sum(ideal_scores / np.log2(np.arange(1, len(ideal_scores) + 1) + 1))\n",
        "        ndcg = dcg / idcg if idcg > 0 else 0\n",
        "\n",
        "        # MRR\n",
        "        mrr = 0\n",
        "        for i, score in enumerate(relevance_scores):\n",
        "            if score > 0:\n",
        "                mrr = 1 / (i + 1)\n",
        "                break\n",
        "\n",
        "        metrics['precision_at_k'].append(precision)\n",
        "        metrics['recall_at_k'].append(recall)\n",
        "        metrics['f1_at_k'].append(f1)\n",
        "        metrics['map_scores'].append(ap)\n",
        "        metrics['ndcg_scores'].append(ndcg)\n",
        "        metrics['mrr_scores'].append(mrr)\n",
        "\n",
        "    # Compute averages\n",
        "    avg_metrics = {\n",
        "        f'avg_{key}': np.mean(values) for key, values in metrics.items()\n",
        "    }\n",
        "\n",
        "    return avg_metrics\n",
        "\n",
        "# Compute metrics\n",
        "print(\"Computing evaluation metrics...\")\n",
        "metrics = compute_graded_metrics(\n",
        "    train_df, query_to_idx, product_to_idx, indices,\n",
        "    unique_queries, unique_products, k=20\n",
        ")"
      ],
      "metadata": {
        "id": "TPSNLtjihFj0"
      },
      "id": "TPSNLtjihFj0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Display Results\n",
        "print(\"\\n=== Evaluation Results ===\")\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "# Step 11: Save Results\n",
        "results_df = pd.DataFrame({\n",
        "    'query': unique_queries,\n",
        "    'top_20_products': [\n",
        "        [unique_products.iloc[idx]['product_id'] for idx in indices[i][:100]]\n",
        "        for i in range(len(unique_queries))\n",
        "    ],\n",
        "    'top_20_titles': [\n",
        "        [unique_products.iloc[idx]['doc_text'] for idx in indices[i][:100]]\n",
        "        for i in range(len(unique_queries))\n",
        "    ],\n",
        "    'similarities': [similarities[i][:100].tolist() for i in range(len(unique_queries))]\n",
        "})"
      ],
      "metadata": {
        "id": "7utYjiqehrRr"
      },
      "id": "7utYjiqehrRr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88082ee8",
      "metadata": {
        "id": "88082ee8"
      },
      "outputs": [],
      "source": [
        "# Save to TSV\n",
        "results_df.to_csv('gpu_esci_results.tsv', sep='\\t', index=False)\n",
        "print(\"\\nResults saved to gpu_esci_results.tsv\")\n",
        "\n",
        "# Save metrics\n",
        "metrics_df = pd.DataFrame([metrics])\n",
        "metrics_df.to_csv('evaluation_metrics.tsv', sep='\\t', index=False)\n",
        "print(\"Metrics saved to evaluation_metrics.tsv\")\n",
        "\n",
        "# Step 12: Memory Cleanup\n",
        "del model\n",
        "del gpu_index\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecbcb9a5",
      "metadata": {
        "id": "ecbcb9a5"
      },
      "outputs": [],
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "bucket_name = 'chanderiyer'\n",
        "destination_blob_name = 'output/metrics/all_mpnet_base_query_eval_metrics.csv'\n",
        "source_file_name = 'all_mpnet_base_query_eval_metrics.csv'\n",
        "\n",
        "storage_client = storage.Client()\n",
        "bucket = storage_client.bucket(bucket_name)\n",
        "blob = bucket.blob(destination_blob_name)\n",
        "\n",
        "minilml6_query_eval_metrics = results_df.to_csv(sep = '\\t', index=False)\n",
        "blob.upload_from_string(minilml6_query_eval_metrics, content_type='text/csv')\n",
        "\n",
        "print(f\"Eval Metrics File {source_file_name} with model {model_name} uploaded to gs://{bucket_name}/{destination_blob_name}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "name": "esci_baseline_sbert_eval_metrics_v2.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}