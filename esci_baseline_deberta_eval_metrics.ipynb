{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c065116e",
      "metadata": {
        "id": "c065116e"
      },
      "source": [
        "\n",
        "# ðŸ“Š Offline Search Evaluation on Amazon ESCI\n",
        "\n",
        "This notebook implements an **offline evaluation prototype** for e-commerce search using the **Amazon ESCI** dataset.\n",
        "- Dataset: [amazon-science/esci-data](https://github.com/amazon-science/esci-data)\n",
        "- Algorithms: BM25 (keyword-based) and SBERT+FAISS (semantic)\n",
        "- Metrics: nDCG@K, MAP@K, MRR, Precision/Recall@K\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import CrossEncoder\n",
        "import torch\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from collections import defaultdict\n",
        "from torch.amp import autocast\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ],
      "metadata": {
        "id": "YecOO5kA5n-V"
      },
      "id": "YecOO5kA5n-V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Define text cleaning function\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Clean text by removing HTML tags, emojis, and special characters.\n",
        "    Args:\n",
        "        text: Input raw text (string).\n",
        "    Returns:\n",
        "        Cleaned text.\n",
        "    \"\"\"\n",
        "    # Remove HTML tags\n",
        "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
        "\n",
        "    # Remove emojis and non-alphanumeric characters\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove emojis and non-ASCII characters\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s:]', '', text)  # Retain alphabetical, numerical, and colon characters\n",
        "\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "OhbUff7WJnuM"
      },
      "id": "OhbUff7WJnuM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_examples = pd.read_parquet('gs://chanderiyer/datasets/esci_shopping/shopping_queries_dataset_examples.parquet')\n",
        "df_products = pd.read_parquet('gs://chanderiyer/datasets/esci_shopping/shopping_queries_dataset_products.parquet')\n",
        "\n",
        "df_products[\"product_description\"] = df_products[\"product_description\"].apply(lambda x: clean_text(x) if not pd.isnull(x) else \"\")\n",
        "df_products[\"product_bullet_point\"] = df_products[\"product_bullet_point\"].apply(lambda x: clean_text(x) if not pd.isnull(x) else \"\")\n",
        "\n",
        "# Combine title, bullet_point, description\n",
        "for c in [\"product_title\",\"product_description\",\"product_bullet_point\",\"product_brand\"]:\n",
        "    if c in df_products.columns:\n",
        "        df_products[c] = df_products[c].fillna(\"\").astype(str)\n",
        "\n",
        "df_products[\"product_full_description\"] = df_products[\"product_title\"] + \" \" + df_products[\"product_bullet_point\"] + \" \" + df_products[\"product_description\"]\n",
        "df_products[\"doc_text\"] = df_products[[\"product_full_description\",\"product_brand\"]].agg(\" \".join, axis=1)\n",
        "\n",
        "df_examples_products = pd.merge(\n",
        "    df_examples,\n",
        "    df_products,\n",
        "    how='inner',\n",
        "    left_on=['product_locale','product_id'],\n",
        "    right_on=['product_locale', 'product_id']\n",
        ")\n",
        "df_examples_products = df_examples_products[(df_examples_products[\"small_version\"] == 1) & (df_examples_products[\"product_locale\"] == \"us\") & (df_examples_products[\"split\"] == \"train\")]"
      ],
      "metadata": {
        "id": "ZMQZ-JK5Jjs1"
      },
      "id": "ZMQZ-JK5Jjs1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = df_examples_products[[\"query_id\", \"query\", \"product_id\", \"doc_text\", \"esci_label\"]].drop_duplicates()\n",
        "train_df.head(n=30)"
      ],
      "metadata": {
        "id": "JSM30Lb_6-Pw"
      },
      "id": "JSM30Lb_6-Pw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Map ESCI labels to numerical scores\n",
        "esci_to_score = {\n",
        "    'E': 3,  # Exact\n",
        "    'S': 2,  # Substitute\n",
        "    'C': 1,  # Complement\n",
        "    'I': 0   # Irrelevant\n",
        "}\n",
        "\n",
        "train_df['relevance_score'] = train_df['esci_label'].map(esci_to_score)\n",
        "train_df.shape"
      ],
      "metadata": {
        "id": "nux6QZGtaymW"
      },
      "id": "nux6QZGtaymW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def create_query_product_pairs_with_sampling(df_examples: pd.DataFrame,\n",
        "#                                             df_products: pd.DataFrame,\n",
        "#                                             sample_size: int = 1000,\n",
        "#                                             random_state: int = 42) -> pd.DataFrame:\n",
        "#     \"\"\"\n",
        "#     Create query-product pairs with random sampling of irrelevant products.\n",
        "\n",
        "#     Args:\n",
        "#         df_examples: DataFrame with existing query-product judgments\n",
        "#         df_products: DataFrame with all products\n",
        "#         sample_size: Number of irrelevant products to sample per query\n",
        "#         random_state: Random seed for reproducibility\n",
        "#     \"\"\"\n",
        "#     np.random.seed(random_state)\n",
        "\n",
        "#     # Get all unique products\n",
        "#     all_products = set(df_products['product_id'].unique())\n",
        "\n",
        "#     # Group existing judgments by query\n",
        "#     query_groups = df_examples.groupby('query_id')\n",
        "\n",
        "#     # Prepare lists for efficient concatenation\n",
        "#     new_pairs = []\n",
        "\n",
        "#     for query_id, group in query_groups:\n",
        "#         # Get labeled products for this query\n",
        "#         labeled_products = set(group['product_id'].unique())\n",
        "\n",
        "#         # Get unlabeled products\n",
        "#         unlabeled_products = list(all_products - labeled_products)\n",
        "\n",
        "#         # Sample unlabeled products\n",
        "#         if len(unlabeled_products) > sample_size:\n",
        "#             sampled_products = np.random.choice(\n",
        "#                 unlabeled_products,\n",
        "#                 size=sample_size,\n",
        "#                 replace=False\n",
        "#             )\n",
        "#         else:\n",
        "#             sampled_products = unlabeled_products\n",
        "\n",
        "#         # Create irrelevant pairs\n",
        "#         if len(sampled_products) > 0:\n",
        "#             irrelevant_pairs = pd.DataFrame({\n",
        "#                 'query_id': query_id,\n",
        "#                 'product_id': sampled_products,\n",
        "#                 'esci_label': 'I',\n",
        "#                 'relevance_score': 0.0\n",
        "#             })\n",
        "#             new_pairs.append(irrelevant_pairs)\n",
        "\n",
        "#     # Combine existing and new pairs\n",
        "#     df_existing = df_examples.copy()\n",
        "\n",
        "#     # Add relevance scores to existing pairs\n",
        "#     label_to_score = {'E': 1.0, 'S': 0.7, 'C': 0.3, 'I': 0.0}\n",
        "#     df_existing['relevance_score'] = df_existing['esci_label'].map(label_to_score)\n",
        "\n",
        "#     # Concatenate all DataFrames\n",
        "#     if new_pairs:\n",
        "#         df_all_pairs = pd.concat([df_existing] + new_pairs, ignore_index=True)\n",
        "#     else:\n",
        "#         df_all_pairs = df_existing\n",
        "\n",
        "#     # Clean up\n",
        "#     del new_pairs\n",
        "#     gc.collect()\n",
        "\n",
        "#     return df_all_pairs\n",
        "\n",
        "# sample_size = 1000\n",
        "# SEED = 42\n",
        "# df_test_small_us_sampled = create_query_product_pairs_with_sampling(df_examples=df_train_small_us, df_products=df_products, sample_size, SEED)\n",
        "# df_test_small_us_sampled.shape"
      ],
      "metadata": {
        "id": "J5aXfP9uVpqv"
      },
      "id": "J5aXfP9uVpqv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Load the Cross-Encoder model with GPU optimization\n",
        "print(\"\\nLoading Cross-Encoder model...\")\n",
        "model = CrossEncoder('cross-encoder/nli-deberta-v3-base', device=device)\n",
        "\n",
        "# Enable mixed precision for the underlying model\n",
        "if hasattr(model.model, 'half') and torch.cuda.is_available():\n",
        "    # For some models, we can use half precision\n",
        "    try:\n",
        "        model.model = model.model.half()\n",
        "        use_fp16 = True\n",
        "        print(\"Using FP16 (half precision) for faster inference\")\n",
        "    except:\n",
        "        use_fp16 = False\n",
        "        print(\"Using FP32 (full precision)\")\n",
        "else:\n",
        "    use_fp16 = False\n",
        "    print(\"Using FP32 (full precision)\")"
      ],
      "metadata": {
        "id": "XkMuVZ_OG_WY"
      },
      "id": "XkMuVZ_OG_WY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ce4ebc2",
      "metadata": {
        "id": "6ce4ebc2"
      },
      "outputs": [],
      "source": [
        "# Step 4: Prepare data for cross-encoder evaluation\n",
        "print(\"\\nPreparing query-product pairs...\")\n",
        "\n",
        "# Group by query to get all products for each query\n",
        "query_products = train_df.groupby(['query_id', 'query']).agg({\n",
        "    'doc_text': list,\n",
        "    'product_id': list,\n",
        "    'relevance_score': list\n",
        "}).reset_index()\n",
        "\n",
        "print(f\"Number of unique queries: {len(query_products)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def score_batch_gpu(model, pairs, batch_size=32):\n",
        "    \"\"\"Score query-product pairs in batches with GPU optimization\"\"\"\n",
        "    scores = []\n",
        "\n",
        "    for i in range(0, len(pairs), batch_size):\n",
        "        batch = pairs[i:i + batch_size]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if use_fp16 and torch.cuda.is_available():\n",
        "                with autocast('cuda'):\n",
        "                    batch_logits = model.predict(batch, apply_softmax=False)\n",
        "            else:\n",
        "                batch_logits = model.predict(batch, apply_softmax=False)\n",
        "\n",
        "        # Assuming batch_logits is (batch_size, num_classes) and index 2 is the 'entailment' score\n",
        "        batch_relevance_scores = np.array(batch_logits)[:, 2]\n",
        "        scores.extend(batch_relevance_scores)\n",
        "\n",
        "        # Clear GPU cache periodically\n",
        "        if i % (batch_size * 10) == 0 and torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return np.array(scores)"
      ],
      "metadata": {
        "id": "XVzeojb6AuZ3"
      },
      "id": "XVzeojb6AuZ3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Evaluate with dynamic batch size optimization\n",
        "print(\"\\nPerforming cross-encoder evaluation with GPU optimization...\")\n",
        "\n",
        "# Dynamic batch size based on GPU memory\n",
        "if torch.cuda.is_available():\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"GPU memory: {gpu_memory:.2f} GB\")\n",
        "    if gpu_memory > 16:\n",
        "        batch_size = 64\n",
        "    elif gpu_memory > 8:\n",
        "        batch_size = 32\n",
        "    else:\n",
        "        batch_size = 16\n",
        "else:\n",
        "    batch_size = 8\n",
        "print(f\"Using batch size: {batch_size}\")\n",
        "\n",
        "results = []\n",
        "total_time = 0\n",
        "\n",
        "# Process queries in batches for memory efficiency\n",
        "query_batch_size = 50\n",
        "num_query_batches = (len(query_products) + query_batch_size - 1) // query_batch_size\n",
        "\n",
        "for batch_idx in tqdm(range(num_query_batches), desc=\"Processing query batches\"):\n",
        "    start_idx = batch_idx * query_batch_size\n",
        "    end_idx = min((batch_idx + 1) * query_batch_size, len(query_products))\n",
        "    batch_queries = query_products.iloc[start_idx:end_idx]\n",
        "\n",
        "    for _, row in batch_queries.iterrows():\n",
        "        query = row[\"query\"]\n",
        "        query_id = row[\"query_id\"]\n",
        "        products = row[\"doc_text\"]\n",
        "        product_ids = row[\"product_id\"]\n",
        "        true_scores = row[\"relevance_score\"]\n",
        "\n",
        "        # Ensure these are lists or NumPy arrays for consistent indexing\n",
        "        products = list(products)\n",
        "        product_ids = list(product_ids)\n",
        "        true_scores = list(true_scores)\n",
        "\n",
        "        # Create query-product pairs\n",
        "        pairs = [(query, product) for product in products]\n",
        "\n",
        "        # Score all pairs with GPU optimization\n",
        "        start_time = time.time()\n",
        "        scores = score_batch_gpu(model, pairs, batch_size)\n",
        "        scoring_time = time.time() - start_time\n",
        "        total_time += scoring_time\n",
        "\n",
        "        # Get top-k products (k=20)\n",
        "        k = min(20, len(products))\n",
        "        # scores is now a 1D array of relevance scores for each product\n",
        "        top_k_indices = np.argsort(scores)[-k:][::-1]\n",
        "\n",
        "        # Store results\n",
        "        result = {\n",
        "            \"query_id\": query_id,\n",
        "            \"query\": query,\n",
        "            \"num_products\": len(products),\n",
        "            \"scoring_time\": scoring_time,\n",
        "            \"top_k_product_ids\": [product_ids[i] for i in top_k_indices],\n",
        "            \"top_k_scores\": [scores[i] for i in top_k_indices], # Scores are already 1D relevance scores\n",
        "            \"top_k_relevance\": [true_scores[i] for i in top_k_indices],\n",
        "            \"all_true_scores\": true_scores\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "    # Clear GPU cache after each batch\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "print(f\"\\nTotal scoring time: {total_time:.2f} seconds\")\n",
        "print(f\"Average time per query: {total_time/len(results):.4f} seconds\")"
      ],
      "metadata": {
        "id": "sHu8oI0_JMHy"
      },
      "id": "sHu8oI0_JMHy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results[:20])"
      ],
      "metadata": {
        "id": "ow4KN0ERZ-r7"
      },
      "id": "ow4KN0ERZ-r7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74c9629b"
      },
      "source": [
        "# Step 7: Compute evaluation metrics\n",
        "print(\"\\nComputing evaluation metrics...\")\n",
        "\n",
        "def compute_metrics(results, k=20):\n",
        "    \"\"\"Compute various IR metrics\"\"\"\n",
        "    metrics = defaultdict(list)\n",
        "\n",
        "    for result in results:\n",
        "        top_k_relevance = result['top_k_relevance'][:k]\n",
        "        all_relevance = result['all_true_scores']\n",
        "\n",
        "        # Binary relevance (relevant if score > 0)\n",
        "        top_k_binary = [1 if r > 0 else 0 for r in top_k_relevance]\n",
        "        all_binary = [1 if r > 0 else 0 for r in all_relevance]\n",
        "\n",
        "        # Precision@k\n",
        "        precision = sum(top_k_binary) / len(top_k_binary) if top_k_binary else 0\n",
        "        metrics['precision@k'].append(precision)\n",
        "\n",
        "        # Recall@k\n",
        "        total_relevant = sum(all_binary)\n",
        "        recall = sum(top_k_binary) / total_relevant if total_relevant > 0 else 0\n",
        "        metrics['recall@k'].append(recall)\n",
        "\n",
        "        # F1@k\n",
        "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "        metrics['f1@k'].append(f1)\n",
        "\n",
        "        # MAP@k (Mean Average Precision)\n",
        "        ap = 0\n",
        "        relevant_count = 0\n",
        "        for i, rel in enumerate(top_k_binary):\n",
        "            if rel:\n",
        "                relevant_count += 1\n",
        "                ap += relevant_count / (i + 1)\n",
        "        map_score = ap / relevant_count if relevant_count > 0 else 0\n",
        "        metrics['map@k'].append(map_score)\n",
        "\n",
        "        # NDCG@k (Normalized Discounted Cumulative Gain)\n",
        "        dcg = sum([(2**rel - 1) / np.log2(i + 2) for i, rel in enumerate(top_k_relevance)])\n",
        "\n",
        "        # Ideal DCG\n",
        "        ideal_relevance = sorted(all_relevance, reverse=True)[:k]\n",
        "        idcg = sum([(2**rel - 1) / np.log2(i + 2) for i, rel in enumerate(ideal_relevance)])\n",
        "\n",
        "        ndcg = dcg / idcg if idcg > 0 else 0\n",
        "        metrics['ndcg@k'].append(ndcg)\n",
        "\n",
        "        # MRR (Mean Reciprocal Rank)\n",
        "        for i, rel in enumerate(top_k_binary):\n",
        "            if rel:\n",
        "                metrics['mrr'].append(1 / (i + 1))\n",
        "                break\n",
        "        else:\n",
        "            metrics['mrr'].append(0)\n",
        "\n",
        "        result['precsion@k'] = precision\n",
        "        result['recall@k'] = recall\n",
        "        result['f1@k'] = f1\n",
        "        result['ndcg@k'] = ndcg\n",
        "        result['map@k'] = map_score\n",
        "        result['mrr'] = metrics['mrr'][-1]\n",
        "\n",
        "    # Average metrics\n",
        "    avg_metrics = {metric: np.mean(scores) for metric, scores in metrics.items()}\n",
        "    return avg_metrics\n",
        "\n",
        "# Compute metrics for different k values\n",
        "# k_values = [5, 10, 20]\n",
        "k_val = 20\n",
        "all_metrics = {}\n",
        "\n",
        "print(f\"\\nMetrics for k={k_val}:\")\n",
        "metrics = compute_metrics(results, k_val)\n",
        "all_metrics[k_val] = metrics\n",
        "\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")"
      ],
      "id": "74c9629b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results[:20])"
      ],
      "metadata": {
        "id": "kaU6vzP2iJHq"
      },
      "id": "kaU6vzP2iJHq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QJLo2jOuEdO0",
      "metadata": {
        "id": "QJLo2jOuEdO0"
      },
      "outputs": [],
      "source": [
        "# Step 8: Save detailed results\n",
        "print(\"\\nSaving results...\")\n",
        "\n",
        "# Create detailed results DataFrame\n",
        "# detailed_results = []\n",
        "# for result in results[:100]:  # Save first 100 queries for inspection\n",
        "#     for i, (product, score, relevance) in enumerate(zip(\n",
        "#         result['top_k_product_ids'][:10],\n",
        "#         result['top_k_scores'][:10],\n",
        "#         result['top_k_relevance'][:10]\n",
        "#     )):\n",
        "#         detailed_results.append({\n",
        "#             'query': result['query'],\n",
        "#             'rank': i + 1,\n",
        "#             'product_title': product,\n",
        "#             'cross_encoder_score': score,\n",
        "#             'true_relevance': relevance,\n",
        "#             'scoring_time_ms': result['scoring_time'] * 1000\n",
        "#         })\n",
        "\n",
        "# detailed_df = pd.DataFrame(detailed_results)\n",
        "# detailed_df.to_csv('cross_encoder_results_gpu.tsv', sep='\\t', index=False)\n",
        "\n",
        "\n",
        "# Save metrics summary\n",
        "metrics_summary = []\n",
        "for k, metrics in all_metrics.items():\n",
        "    for metric, value in metrics.items():\n",
        "        metrics_summary.append({\n",
        "            'k': k,\n",
        "            'metric': metric,\n",
        "            'value': value\n",
        "        })"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "bucket_name = 'chanderiyer'\n",
        "summary_destination_blob_name = 'output/metrics/debertav3base_summary_eval_metrics.csv'\n",
        "summary_source_file_name = 'debertav3base_summary_eval_metrics.csv'\n",
        "\n",
        "storage_client = storage.Client()\n",
        "bucket = storage_client.bucket(bucket_name)\n",
        "summary_blob = bucket.blob(summary_destination_blob_name)\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics_summary)\n",
        "model_summary_eval_metrics = metrics_df.to_csv(sep = '\\t', index=False)\n",
        "\n",
        "summary_blob.upload_from_string(model_summary_eval_metrics, content_type='text/csv')\n",
        "print(f\"Model weights {summary_source_file_name} uploaded to gs://{bucket_name}/{summary_destination_blob_name}\")\n",
        "\n",
        "\n",
        "query_destination_blob_name = 'output/metrics/debertav3base_query_eval_metrics.csv'\n",
        "query_source_file_name = 'debertav3base_query_eval_metrics.csv'\n",
        "query_blob = bucket.blob(query_destination_blob_name)\n",
        "\n",
        "query_metrics_df = pd.DataFrame(results)\n",
        "model_query_eval_metrics = query_metrics_df.to_csv(sep='\\t', index=False)\n",
        "\n",
        "query_blob.upload_from_string(model_query_eval_metrics, content_type='text/csv')\n",
        "print(f\"Model weights {query_source_file_name} uploaded to gs://{bucket_name}/{query_destination_blob_name}\")\n",
        "\n",
        "\n",
        "print(\"\\nResults saved to:\")\n",
        "print(\"- cross_encoder_results_gpu.tsv (detailed results)\")\n",
        "print(\"- cross_encoder_metrics_gpu.tsv (metrics summary)\")\n",
        "\n",
        "# Performance comparison\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"PERFORMANCE SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Total queries processed: {len(results)}\")\n",
        "print(f\"Total time: {total_time:.2f} seconds\")\n",
        "print(f\"Average time per query: {total_time/len(results):.4f} seconds\")\n",
        "print(f\"Queries per second: {len(results)/total_time:.2f}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\\nGPU Memory Usage: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")"
      ],
      "metadata": {
        "id": "HzanFqj3hafY"
      },
      "id": "HzanFqj3hafY",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "name": "esci_baseline_deberta_eval_metrics.ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}