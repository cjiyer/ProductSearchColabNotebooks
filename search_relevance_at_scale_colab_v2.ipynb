{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Search Relevance at Scale \u2014 Vespa vs FAISS, LightGBM vs PyTorch *(+ MS MARCO + FastAPI)*\n\n**Dataset toggle** (Amazon Reviews Multi or MS MARCO), hybrid retrieval (BM25 + FAISS e5), RRF, LightGBM vs PyTorch ranking, optional CE rerank, Vespa scaffold, and **FastAPI** online scoring."}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "\n#@title Install dependencies\n%%capture\n!pip -q install --upgrade pip\n!pip -q install datasets transformers sentence-transformers faiss-cpu rank-bm25                  lightgbm scikit-learn torchmetrics pyvespa pandas numpy matplotlib tqdm                  langdetect unidecode scipy fastapi uvicorn pydantic"}, {"cell_type": "markdown", "metadata": {}, "source": "## 1) Imports & Config"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "\nimport os, time, json, math, re, random, itertools\nimport numpy as np, pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import entropy\nimport torch, torch.nn as nn, torch.optim as optim\nfrom datasets import load_dataset\nfrom sentence_transformers import SentenceTransformer\nfrom rank_bm25 import BM25Okapi\nimport faiss, lightgbm as lgb\nfrom tqdm.auto import tqdm\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nSEED = 42\nnp.random.seed(SEED); torch.manual_seed(SEED); random.seed(SEED)\nCONFIG = {\"DATASET\":\"amazon\",\"language\":\"en\",\"N_DOCS\":60000,\"N_QUERIES\":8000,\"TOPK_BM25\":200,\"TOPK_ANN\":200,\"FUSION_K\":300,\"RERANK_TOPN\":40,\"USE_CE_RERANK\":False}\nCONFIG"}, {"cell_type": "markdown", "metadata": {}, "source": "## 2) Dataset Loaders \u2014 Amazon Reviews Multi and MS MARCO"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "\ndef load_amazon_reviews_multi(lang=\"en\", n_docs=60000, n_queries=8000, seed=SEED):\n    ds = load_dataset(\"amazon_reviews_multi\", lang, split=\"train\")\n    df = ds.to_pandas()[[\"product_id\",\"review_title\",\"review_body\",\"stars\"]].dropna()\n    g = df.groupby(\"product_id\")\n    prod = g.agg({\"review_title\": lambda s: \" | \".join(s.head(10).astype(str)),\n                  \"review_body\":  lambda s: \" \".join(s.head(5).astype(str)),\n                  \"stars\": \"mean\"}).reset_index()\n    prod[\"doc_text\"] = (prod[\"review_title\"].fillna(\"\") + \" \" + prod[\"review_body\"].fillna(\"\")).str.strip()\n    prod = prod[prod[\"doc_text\"].str.len()>32].sample(frac=1, random_state=seed).head(n_docs).reset_index(drop=True)\n    pids = set(prod[\"product_id\"])\n    q = df[df[\"product_id\"].isin(pids)][[\"review_title\",\"product_id\"]].dropna()\n    q = q.rename(columns={\"review_title\":\"query\",\"product_id\":\"relevant_pid\"}).drop_duplicates()\n    q = q.sample(frac=1, random_state=seed).head(n_queries).reset_index(drop=True)\n    prod = prod[[\"product_id\",\"doc_text\",\"stars\"]]\n    return prod, q\n\ndef load_msmarco(n_docs=120000, n_queries=12000, seed=SEED):\n    ds = load_dataset(\"ms_marco\", \"v2.1\", split=\"train[:10%]\")\n    docs, qrows, seen = [], [], {}\n    for r in ds:\n        q = r[\"query\"]; passages = r.get(\"passages\", {})\n        pos_idxs = [i for i,sel in enumerate(passages.get(\"is_selected\", [])) if sel==1]\n        if not pos_idxs: continue\n        pid_text = passages[\"passage_text\"][pos_idxs[0]]\n        did = str(abs(hash(pid_text)) % (10**12))\n        if did not in seen:\n            seen[did] = {\"product_id\": did, \"doc_text\": pid_text, \"stars\": 0.0}\n            docs.append(seen[did])\n        qrows.append({\"query\": q, \"relevant_pid\": did})\n        if len(docs) >= n_docs and len(qrows) >= n_queries: break\n    docs_df = pd.DataFrame(docs).sample(frac=1, random_state=seed).head(n_docs).reset_index(drop=True)\n    queries_df = pd.DataFrame(qrows).sample(frac=1, random_state=seed).head(n_queries).reset_index(drop=True)\n    return docs_df, queries_df\n\ndocs_df, queries_df = (load_amazon_reviews_multi(CONFIG[\"language\"], CONFIG[\"N_DOCS\"], CONFIG[\"N_QUERIES\"])\n                       if CONFIG[\"DATASET\"]==\"amazon\" else\n                       load_msmarco(CONFIG[\"N_DOCS\"], CONFIG[\"N_QUERIES\"]))\nlen(docs_df), len(queries_df)"}, {"cell_type": "markdown", "metadata": {}, "source": "## 3) BM25 + FAISS (e5) + RRF"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "\nimport re\ndef simple_tokenize(txt): return [t for t in re.sub(r\"\\W+\",\" \", str(txt).lower()).split() if t]\nbm25 = BM25Okapi([simple_tokenize(t) for t in docs_df[\"doc_text\"].tolist()])\n\ndef bm25_search(qs, k):\n    out_idx, out_scores = [], []\n    for q in qs:\n        s = bm25.get_scores(simple_tokenize(q))\n        top = np.argpartition(s, -k)[-k:]; top = top[np.argsort(-s[top])]\n        out_idx.append(top); out_scores.append(s[top])\n    return out_idx, out_scores\n\ndense = SentenceTransformer(\"intfloat/multilingual-e5-base\", device=DEVICE)\ndef encode_texts(texts, bs=128, normal=True):\n    vecs=[]; \n    for i in range(0,len(texts),bs):\n        emb = dense.encode(texts[i:i+bs], batch_size=bs, convert_to_numpy=True, normalize_embeddings=normal, show_progress_bar=False)\n        vecs.append(emb.astype(\"float32\"))\n    return np.vstack(vecs)\n\nimport faiss\ndoc_vec = encode_texts(docs_df[\"doc_text\"].tolist(), 128, True)\nindex = faiss.IndexHNSWFlat(doc_vec.shape[1], 32); index.hnsw.efConstruction=200; index.hnsw.efSearch=128; index.add(doc_vec)\ndef ann_search(qs, k):\n    q_vec = encode_texts(qs, 128, True); sco, idx = index.search(q_vec, k); return idx, sco\n\ndef rrf_fusion(bm_idx, ann_idx, k=300, K=60):\n    fused=[]; \n    for i in range(len(bm_idx)):\n        scores={}\n        for r,d in enumerate(bm_idx[i]): scores[d]=scores.get(d,0)+1.0/(K+r+1)\n        for r,d in enumerate(ann_idx[i]): scores[d]=scores.get(d,0)+1.0/(K+r+1)\n        top=sorted(scores.items(), key=lambda x:-x[1])[:k]\n        fused.append(np.array([d for d,_ in top], dtype=int))\n    return fused"}, {"cell_type": "markdown", "metadata": {}, "source": "## 4) Candidates, Features, Labels"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "\ntrain_q, dev_q = train_test_split(queries_df, test_size=0.2, random_state=SEED, shuffle=True)\ndef build_candidates_and_labels(qdf):\n    qs = qdf[\"query\"].tolist()\n    bm_idx,_ = bm25_search(qs, 200)\n    ann_idx,_ = ann_search(qs, 200)\n    fused = rrf_fusion(bm_idx, ann_idx, 300)\n    feats, labels, groups = [], [], []\n    for i,row in enumerate(qdf.itertuples(index=False)):\n        rel_pid = str(row.relevant_pid)\n        bm_pos = {d:r for r,d in enumerate(bm_idx[i])}\n        an_pos = {d:r for r,d in enumerate(ann_idx[i])}\n        q_feats=[]; q_labels=[]\n        for d in fused[i]:\n            bmr = bm_pos.get(d, 9999); anr = an_pos.get(d, 9999)\n            pop = float(docs_df.iloc[d][\"stars\"]) if \"stars\" in docs_df.columns else 0.0\n            dlen = len(str(docs_df.iloc[d][\"doc_text\"]).split())\n            q_feats.append([bmr, anr, pop, dlen])\n            q_labels.append(1.0 if str(docs_df.iloc[d][\"product_id\"])==rel_pid else 0.0)\n        if sum(q_labels)==0: continue\n        feats.append(np.array(q_feats, np.float32)); labels.append(np.array(q_labels, np.float32)); groups.append(len(q_feats))\n    return feats, labels, groups\n\nXtr_list, ytr_list, gtr = build_candidates_and_labels(train_q)\nXdv_list, ydv_list, gdv = build_candidates_and_labels(dev_q)\nlen(Xtr_list), len(Xdv_list)"}, {"cell_type": "markdown", "metadata": {}, "source": "## 5) Rankers \u2014 LightGBM LambdaMART & PyTorch LambdaRank"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "\ndef flatten_for_lgb(X_list, y_list):\n    return np.concatenate(X_list,0), np.concatenate(y_list,0)\nXtr, ytr = flatten_for_lgb(Xtr_list, ytr_list); Xdv, ydv = flatten_for_lgb(Xdv_list, ydv_list)\nlgb_train = lgb.Dataset(Xtr, label=ytr, group=gtr); lgb_valid = lgb.Dataset(Xdv, label=ydv, group=gdv, reference=lgb_train)\nparams = {\"objective\":\"lambdarank\",\"metric\":\"ndcg\",\"ndcg_at\":[10],\"learning_rate\":0.05,\"num_leaves\":63,\"min_data_in_leaf\":50,\"feature_pre_filter\":False,\"verbose\":-1}\nlgb_model = lgb.train(params, lgb_train, valid_sets=[lgb_valid], num_boost_round=300, early_stopping_rounds=30, verbose_eval=50)"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "\nclass LambdaRankTorch(nn.Module):\n    def __init__(self, in_dim): super().__init__(); self.mlp = nn.Sequential(nn.Linear(in_dim,128), nn.ReLU(), nn.Linear(128,1))\n    def forward(self,x): return self.mlp(x)\ndef pairwise_loss(scores, labels):\n    s=scores.view(-1); y=labels.view(-1); pos=torch.where(y>0.5)[0]; neg=torch.where(y<0.5)[0]\n    if len(pos)==0 or len(neg)==0: return None\n    diff = s[pos].unsqueeze(1) - s[neg].unsqueeze(0); return torch.mean(torch.log1p(torch.exp(-diff)))\ndef train_lambdarank_torch(X_list, y_list, epochs=3, lr=3e-3, device=DEVICE):\n    model=LambdaRankTorch(in_dim=X_list[0].shape[1]).to(device); opt=optim.AdamW(model.parameters(), lr=lr)\n    for ep in range(epochs):\n        tot=0; cnt=0; model.train()\n        for Xq,yq in zip(X_list,y_list):\n            xb=torch.tensor(Xq, dtype=torch.float32).to(device); yb=torch.tensor(yq, dtype=torch.float32).to(device)\n            opt.zero_grad(); sc=model(xb); l=pairwise_loss(sc,yb); \n            if l is None: continue\n            l.backward(); opt.step(); tot+=l.item(); cnt+=1\n        print(f\"[Torch] epoch {ep} loss {tot/max(cnt,1):.4f}\")\n    return model\ntorch_model = train_lambdarank_torch(Xtr_list, ytr_list, epochs=3, lr=3e-3)"}, {"cell_type": "markdown", "metadata": {}, "source": "## 6) Evaluation + Artifact Save (for FastAPI)"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "\ndef ndcg_at_k(labels, scores, k=10):\n    order = np.argsort(-scores)[:k]; rel = np.array(labels)[order]\n    gains = (2**rel - 1)/np.log2(np.arange(2,len(rel)+2))\n    ideal = (2**sorted(labels, reverse=True) - 1)/np.log2(np.arange(2,len(rel)+2))\n    ideal = ideal.sum() if len(ideal)>0 else 1.0\n    return gains.sum()/ideal if ideal>0 else 0.0\n\ndef eval_ranker_lightgbm(X_list, y_list, model):\n    ndcgs=[]; rec=[]\n    for Xq,yq in zip(X_list,y_list):\n        sc=model.predict(Xq, num_iteration=model.best_iteration)\n        ndcgs.append(ndcg_at_k(yq, sc, 10)); rec.append(int(np.sum(np.array(yq)[np.argsort(-sc)[:100]])>0))\n    return float(np.mean(ndcgs)), float(np.mean(rec))\n\ndef eval_ranker_torch(X_list, y_list, model):\n    model.eval(); ndcgs=[]; rec=[]\n    with torch.no_grad():\n        for Xq,yq in zip(X_list,y_list):\n            sc=model(torch.tensor(Xq, dtype=torch.float32).to(DEVICE)).cpu().numpy().ravel()\n            ndcgs.append(ndcg_at_k(yq, sc, 10)); rec.append(int(np.sum(np.array(yq)[np.argsort(-sc)[:100]])>0))\n    return float(np.mean(ndcgs)), float(np.mean(rec))\n\nprint(\"LightGBM:\", eval_ranker_lightgbm(Xdv_list, ydv_list, lgb_model))\nprint(\"PyTorch :\", eval_ranker_torch(Xdv_list, ydv_list, torch_model))\n\nART_DIR = \"/content/artifacts\"; os.makedirs(ART_DIR, exist_ok=True)\ndocs_meta = {\"product_id\": docs_df[\"product_id\"].astype(str).tolist(),\n             \"doc_text\": docs_df[\"doc_text\"].tolist(),\n             \"stars\": docs_df[\"stars\"].tolist() if \"stars\" in docs_df.columns else [0.0]*len(docs_df)}\nopen(os.path.join(ART_DIR,\"docs_meta.json\"),\"w\").write(json.dumps(docs_meta))\nfaiss.write_index(index, os.path.join(ART_DIR,\"faiss_hnsw.index\"))\nlgb_model.save_model(os.path.join(ART_DIR,\"lgb_lambdamart.txt\"), num_iteration=lgb_model.best_iteration)\ntorch.save({\"state_dict\": torch_model.state_dict(), \"in_dim\": Xtr_list[0].shape[1]}, os.path.join(ART_DIR,\"torch_lambdarank.pt\"))\nopen(os.path.join(ART_DIR,\"dense_model.json\"),\"w\").write(json.dumps({\"name\":\"intfloat/multilingual-e5-base\"}))\nopen(os.path.join(ART_DIR,\"config.json\"),\"w\").write(json.dumps(CONFIG))\nos.listdir(ART_DIR)"}], "metadata": {"colab": {"name": "Search Relevance at Scale \u2014 Vespa/FAISS + LightGBM/PyTorch + MS MARCO + FastAPI", "provenance": []}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "nbformat": 4, "nbformat_minor": 5}