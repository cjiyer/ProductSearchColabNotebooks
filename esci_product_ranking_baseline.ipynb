{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "!pip install transformers datasets scikit-learn numpy pandas tqdm matplotlib"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, XLMRobertaForSequenceClassification, AdamW, get_scheduler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "id": "b42a6ff5d05bc9af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# File path to the ESCI Dataset\n",
    "data_path = \"path/to/esci_dataset.csv\"  # Replace with your actual dataset path\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Example columns in ESCI dataset: query, product_title, label (E=3, S=2, C=1, I=0)\n",
    "print(df.head())\n",
    "\n",
    "# Split into train, validation, and test sets\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}, Validation samples: {len(val_df)}, Test samples: {len(test_df)}\")"
   ],
   "id": "e9a26bfa0b7cc98d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def smooth_labels(labels, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Applies label smoothing to reduce the impact of noisy labels.\n",
    "    Args:\n",
    "        labels: Array of integer labels (E=3, S=2, C=1, I=0).\n",
    "        epsilon: Smoothing factor (default: 0.1).\n",
    "    Returns:\n",
    "        smoothed_labels: Array of smoothed label probabilities.\n",
    "    \"\"\"\n",
    "    num_classes = 4  # E=3, S=2, C=1, I=0\n",
    "    smoothed_labels = (1 - epsilon) * labels + epsilon / num_classes\n",
    "    return smoothed_labels\n",
    "\n",
    "# Apply label smoothing to the dataset\n",
    "train_df[\"smoothed_label\"] = smooth_labels(train_df[\"label\"])\n",
    "val_df[\"smoothed_label\"] = smooth_labels(val_df[\"label\"])\n",
    "test_df[\"smoothed_label\"] = smooth_labels(test_df[\"label\"])"
   ],
   "id": "3bb312e80d243cf7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class AugmentedESCDataset(Dataset):\n",
    "    def __init__(self, queries, products, labels, pseudo_labels, tokenizer, max_len=128):\n",
    "        self.queries = queries\n",
    "        self.products = products\n",
    "        self.labels = labels\n",
    "        self.pseudo_labels = pseudo_labels  # Include pseudo labels derived from the model's outputs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.queries)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        query = str(self.queries[item])\n",
    "        product = str(self.products[item])\n",
    "        label = self.labels[item]\n",
    "        pseudo_label = self.pseudo_labels[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            query,\n",
    "            product,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"label\": torch.tensor(label, dtype=torch.float),\n",
    "            \"pseudo_label\": torch.tensor(pseudo_label, dtype=torch.float)\n",
    "        }"
   ],
   "id": "bb82a05663d705fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def apply_adversarial_weight_perturbation(model, embeddings, epsilon=0.01, perturb_steps=1):\n",
    "    \"\"\"\n",
    "    Applies adversarial weight perturbation to the embedding layer.\n",
    "    Args:\n",
    "        model: Pretrained transformer model (XLM-RoBERTa).\n",
    "        embeddings: Embedding weights.\n",
    "        epsilon: Magnitude of perturbation (default: 0.01).\n",
    "        perturb_steps: Number of steps for perturbation (default: 1).\n",
    "    Returns:\n",
    "        perturbed_embeddings: Perturbed embedding weights.\n",
    "    \"\"\"\n",
    "    perturbed_embeddings = embeddings.clone()\n",
    "    for _ in range(perturb_steps):\n",
    "        perturbation = epsilon * torch.randn_like(embeddings)\n",
    "        perturbed_embeddings += perturbation\n",
    "        perturbed_embeddings = nn.functional.normalize(perturbed_embeddings, dim=-1)  # Normalize perturbation\n",
    "\n",
    "    return perturbed_embeddings"
   ],
   "id": "8d3df8eb65686bab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Training loop with self-distillation and adversarial weight perturbation\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Load data to device\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].unsqueeze(1).to(device)\n",
    "        pseudo_labels = batch[\"pseudo_label\"].unsqueeze(1).to(device)\n",
    "\n",
    "        # Forward pass with regular embeddings\n",
    "        outputs_regular = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions_regular = outputs_regular.logits\n",
    "\n",
    "        # Apply adversarial perturbation to weights\n",
    "        embeddings = model.base_model.embeddings.word_embeddings.weight\n",
    "        perturbed_embeddings = apply_adversarial_weight_perturbation(model, embeddings)\n",
    "        model.base_model.embeddings.word_embeddings.weight = nn.Parameter(perturbed_embeddings)\n",
    "\n",
    "        # Forward pass with adversarial embeddings\n",
    "        outputs_perturbed = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions_perturbed = outputs_perturbed.logits\n",
    "\n",
    "        # Combine loss: Regular loss + Pseudo-label loss + Perturbed loss\n",
    "        loss_regular = loss_fn(predictions_regular, labels)\n",
    "        loss_pseudo = loss_fn(predictions_regular, pseudo_labels)\n",
    "        loss_perturbed = loss_fn(predictions_perturbed, labels)\n",
    "\n",
    "        # Total loss\n",
    "        loss = (loss_regular + loss_pseudo + loss_perturbed) / 3\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model weights\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch Loss: {epoch_loss / len(train_loader)}\")"
   ],
   "id": "f4e5e4b222679169"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the XLM-RoBERTa tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "# Create DataLoader objects for training and validation\n",
    "train_dataset = ESCDataset(train_df[\"query\"].values, train_df[\"product_title\"].values, train_df[\"label\"].values, tokenizer)\n",
    "val_dataset = ESCDataset(val_df[\"query\"].values, val_df[\"product_title\"].values, val_df[\"label\"].values, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)"
   ],
   "id": "d3a73013b54883db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load XLM-RoBERTa for sequence classification\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=1)  # Regression task\n",
    "model = model.to(device)\n",
    "\n",
    "# Define optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "num_training_steps = len(train_loader) * 3  # Assuming 3 epochs\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = torch.nn.MSELoss()  # For regression tasks\n",
    "\n",
    "# Training Loop\n",
    "epochs = 3\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move batch data to device\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].unsqueeze(1).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = outputs.logits\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(predictions, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model weights\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch Loss: {epoch_loss / len(train_loader)}\")"
   ],
   "id": "8a3a8ddd1e5cf66f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    relevance_scores = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            true_labels.extend(labels.tolist())\n",
    "            relevance_scores.extend(torch.sigmoid(outputs.logits).tolist())\n",
    "\n",
    "    return relevance_scores, true_labels\n",
    "\n",
    "# Evaluate on validation and test sets\n",
    "val_scores, val_labels = evaluate_model(model, val_loader, device)\n",
    "test_scores, test_labels = evaluate_model(model, test_loader, device)"
   ],
   "id": "88ccd6cfd23f131"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def compute_metrics(true_labels, relevance_scores):\n",
    "    mse = mean_squared_error(true_labels, relevance_scores)\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Compute metrics for validation set\n",
    "compute_metrics(val_labels, val_scores)"
   ],
   "id": "f51996cee95d87ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Example data for NDCG computation\n",
    "k = 10\n",
    "query_ids = test_df[\"query_id\"].unique()\n",
    "\n",
    "# Compute NDCG@k for test queries\n",
    "ndcg_results = []\n",
    "for query_id in query_ids:\n",
    "    query_products = test_df[test_df[\"query_id\"] == query_id]\n",
    "    relevance_scores = query_products[\"model_score\"].values\n",
    "    # Sort products by relevance scores\n",
    "    relevance_ranking = np.argsort(relevance_scores)[::-1]  # Highest scores first\n",
    "    relevance_sorted_scores = [relevance_scores[i] for i in relevance_ranking]\n",
    "\n",
    "    ndcg_results.append(ndcg_at_k(relevance_sorted_scores, k))\n",
    "\n",
    "average_ndcg = sum(ndcg_results) / len(ndcg_results)\n",
    "print(f\"Average NDCG@{k}: {average_ndcg}\")"
   ],
   "id": "847c40aaa72f93c7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
