{"cells":[{"cell_type":"markdown","metadata":{"id":"s2SRtt0Y3YUi"},"source":["# Search Relevance at Scale — Vespa vs FAISS, LightGBM vs PyTorch\n","\n","This notebook is **Colab-executable** and walks through a scalable search relevance pipeline:\n","- **Data**: Amazon Reviews Multi (medium/large, multilingual).\n","- **Retrieval**: BM25 (lexical) + **FAISS HNSW** (dense, multilingual e5).\n","- **Fusion**: RRF.\n","- **Ranking**: **LightGBM LambdaMART** vs **PyTorch LambdaRank** (pairwise).\n","- **(Optional)** Rerank: Cross-Encoder (MiniLM) on top-*N*.\n","- **(Optional)** Vespa: build a minimal **application package**; deploy if you have Docker/endpoint. Colab cannot run Vespa locally, but this notebook generates the package and shows the feed/query code you can reuse with a running Vespa instance.\n","\n","**Metrics**: NDCG@10, Recall@100, zero-result rate, simple latency sampling.  \n","**Monitoring**: feature drift (PSI), query mix entropy, quick interleaving simulation."],"id":"s2SRtt0Y3YUi"},{"cell_type":"code","metadata":{"id":"jO4n0RlA3YUl"},"execution_count":null,"outputs":[],"source":["\n","#@title Install dependencies\n","%%capture\n","!pip -q install --upgrade pip\n","!pip -q install datasets transformers sentence-transformers faiss-cpu rank-bm25                  lightgbm scikit-learn torchmetrics pyvespa pandas numpy matplotlib tqdm                  langdetect unidecode scipy"],"id":"jO4n0RlA3YUl"},{"cell_type":"markdown","metadata":{"id":"oUBLz6GS3YUm"},"source":["## 1) Imports & Config"],"id":"oUBLz6GS3YUm"},{"cell_type":"code","metadata":{"id":"MpwjvHDl3YUm"},"execution_count":null,"outputs":[],"source":["\n","import os, time, json, math, re, random, itertools\n","import numpy as np, pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import torch, torch.nn as nn, torch.optim as optim\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import roc_auc_score\n","from scipy.stats import entropy\n","\n","from datasets import load_dataset\n","from sentence_transformers import SentenceTransformer\n","from rank_bm25 import BM25Okapi\n","import faiss\n","import lightgbm as lgb\n","\n","from tqdm.auto import tqdm\n","from langdetect import detect as lang_detect\n","from unidecode import unidecode\n","\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","SEED = 42\n","np.random.seed(SEED); torch.manual_seed(SEED); random.seed(SEED)\n","\n","CONFIG = {\n","    \"language\": \"en\",        # any of: 'en','de','fr','ja','zh','es','it'...\n","    \"N_DOCS\": 60000,         # scale up if Colab allows (memory/ runtime)\n","    \"N_QUERIES\": 8000,\n","    \"TOPK_BM25\": 200,\n","    \"TOPK_ANN\": 200,\n","    \"FUSION_K\": 300,\n","    \"RERANK_TOPN\": 40,       # used only if you enable cross-encoder rerank\n","    \"USE_CE_RERANK\": False   # set True to enable (slower)\n","}"],"id":"MpwjvHDl3YUm"},{"cell_type":"markdown","metadata":{"id":"6dTcZLMB3YUn"},"source":["## 2) Load & Build a Product Corpus + Queries"],"id":"6dTcZLMB3YUn"},{"cell_type":"code","metadata":{"id":"Kz2zQ60R3YUn"},"execution_count":null,"outputs":[],"source":["\n","def load_amazon_reviews_multi(lang=\"en\", n_docs=60000, n_queries=8000, seed=SEED):\n","    # Load split of Amazon Reviews Multi in the given language\n","    ds = load_dataset(\"amazon_reviews_multi\", lang, split=\"train\")\n","    df = ds.to_pandas()[[\"product_id\",\"review_title\",\"review_body\",\"stars\"]].dropna()\n","    # Aggregate reviews per product into a \"document\"\n","    g = df.groupby(\"product_id\")\n","    prod = g.agg({\n","        \"review_title\": lambda s: \" | \".join(s.head(10).astype(str)),\n","        \"review_body\":  lambda s: \" \".join(s.head(5).astype(str)),\n","        \"stars\": \"mean\"\n","    }).reset_index()\n","    prod[\"doc_text\"] = (prod[\"review_title\"].fillna(\"\") + \" \" + prod[\"review_body\"].fillna(\"\")).str.strip()\n","    prod = prod[prod[\"doc_text\"].str.len() > 32].sample(frac=1, random_state=seed).head(n_docs).reset_index(drop=True)\n","\n","    # Build a weak-labeled query set from titles; align relevant_pid to the same product\n","    pids = set(prod[\"product_id\"])\n","    q_df = df[df[\"product_id\"].isin(pids)][[\"review_title\",\"product_id\"]].dropna()\n","    q_df = q_df.rename(columns={\"review_title\":\"query\",\"product_id\":\"relevant_pid\"}).drop_duplicates()\n","    q_df = q_df.sample(frac=1, random_state=seed).head(n_queries).reset_index(drop=True)\n","\n","    # Keep only necessary columns\n","    prod = prod[[\"product_id\",\"doc_text\",\"stars\"]]\n","    return prod, q_df\n","\n","docs_df, queries_df = load_amazon_reviews_multi(CONFIG[\"language\"], CONFIG[\"N_DOCS\"], CO~NFIG[\"N_QUERIES\"])\n","docs_df.head(), queries_df.head(), len(docs_df), len(queries_df)"],"id":"Kz2zQ60R3YUn"},{"cell_type":"markdown","metadata":{"id":"zMCePQtK3YUn"},"source":["## 3) Lexical Retrieval (BM25/OpenSearch-like)"],"id":"zMCePQtK3YUn"},{"cell_type":"code","metadata":{"id":"fAU5gX9c3YUn"},"execution_count":null,"outputs":[],"source":["\n","def simple_tokenize(txt):\n","    txt = str(txt).lower().replace(\"\\n\", \" \")\n","    return [t for t in re.sub(r\"\\W+\", \" \", txt).split() if t]\n","\n","bm25 = BM25Okapi([simple_tokenize(t) for t in docs_df[\"doc_text\"].tolist()])\n","\n","def bm25_search(qs, k):\n","    out_idx, out_scores = [], []\n","    for q in qs:\n","        s = bm25.get_scores(simple_tokenize(q))\n","        top = np.argpartition(s, -k)[-k:]\n","        top = top[np.argsort(-s[top])]\n","        out_idx.append(top)\n","        out_scores.append(s[top])\n","    return out_idx, out_scores"],"id":"fAU5gX9c3YUn"},{"cell_type":"markdown","metadata":{"id":"QSMzJjZJ3YUo"},"source":["## 4) Dense Retrieval (SentenceTransformers e5 + FAISS HNSW)"],"id":"QSMzJjZJ3YUo"},{"cell_type":"code","metadata":{"id":"Kg6xDo8J3YUo"},"execution_count":null,"outputs":[],"source":["\n","dense = SentenceTransformer(\"intfloat/multilingual-e5-base\", device=DEVICE)\n","\n","def encode_texts(texts, bs=128, normal=True):\n","    vecs = []\n","    for i in range(0, len(texts), bs):\n","        emb = dense.encode(\n","            texts[i:i+bs],\n","            batch_size=bs,\n","            convert_to_numpy=True,\n","            normalize_embeddings=normal,\n","            show_progress_bar=False\n","        )\n","        vecs.append(emb.astype(\"float32\"))\n","    return np.vstack(vecs)\n","\n","doc_vec = encode_texts(docs_df[\"doc_text\"].tolist(), bs=128, normal=True)\n","index = faiss.IndexHNSWFlat(doc_vec.shape[1], 32)\n","index.hnsw.efConstruction = 200\n","index.hnsw.efSearch = 128\n","index.add(doc_vec)\n","\n","def ann_search(qs, k):\n","    q_vec = encode_texts(qs, bs=128, normal=True)\n","    sco, idx = index.search(q_vec, k)\n","    return idx, sco"],"id":"Kg6xDo8J3YUo"},{"cell_type":"markdown","metadata":{"id":"m0xCUQny3YUo"},"source":["## 5) Candidate Fusion (RRF)"],"id":"m0xCUQny3YUo"},{"cell_type":"code","metadata":{"id":"VGPlsV5T3YUo"},"execution_count":null,"outputs":[],"source":["\n","def rrf_fusion(bm_idx, ann_idx, k=300, K=60):\n","    fused = []\n","    for i in range(len(bm_idx)):\n","        scores = {}\n","        for r, d in enumerate(bm_idx[i]):\n","            scores[d] = scores.get(d, 0.0) + 1.0/(K + r + 1)\n","        for r, d in enumerate(ann_idx[i]):\n","            scores[d] = scores.get(d, 0.0) + 1.0/(K + r + 1)\n","        top = sorted(scores.items(), key=lambda x: -x[1])[:k]\n","        fused.append(np.array([d for d,_ in top], dtype=int))\n","    return fused"],"id":"VGPlsV5T3YUo"},{"cell_type":"markdown","metadata":{"id":"xkWLoF3d3YUo"},"source":["## 6) Build Candidates & Labels"],"id":"xkWLoF3d3YUo"},{"cell_type":"code","metadata":{"id":"AaqAXO983YUo"},"execution_count":null,"outputs":[],"source":["\n","train_q, dev_q = train_test_split(queries_df, test_size=0.2, random_state=SEED, shuffle=True)\n","\n","def build_candidates_and_labels(qdf):\n","    qs = qdf[\"query\"].tolist()\n","    bm_idx, bm_s = bm25_search(qs, CONFIG[\"TOPK_BM25\"])\n","    ann_idx, ann_s = ann_search(qs, CONFIG[\"TOPK_ANN\"])\n","    fused = rrf_fusion(bm_idx, ann_idx, CONFIG[\"FUSION_K\"])\n","\n","    feats, labels, groups = [], [], []\n","    for i, row in enumerate(qdf.itertuples(index=False)):\n","        rel_pid = row.relevant_pid\n","        q_feats, q_labels = [], []\n","        bm_positions = {d: r for r, d in enumerate(bm_idx[i])}\n","        ann_positions = {d: r for r, d in enumerate(ann_idx[i])}\n","        for d in fused[i]:\n","            bmr = bm_positions.get(d, 9999)\n","            anr = ann_positions.get(d, 9999)\n","            pop = float(docs_df.iloc[d][\"stars\"])\n","            dlen = len(str(docs_df.iloc[d][\"doc_text\"]).split())\n","            q_feats.append([bmr, anr, pop, dlen])\n","            q_labels.append(1.0 if docs_df.iloc[d][\"product_id\"] == rel_pid else 0.0)\n","        if sum(q_labels) == 0:\n","            continue\n","        feats.append(np.array(q_feats, dtype=np.float32))\n","        labels.append(np.array(q_labels, dtype=np.float32))\n","        groups.append(len(q_feats))\n","    return feats, labels, groups\n","\n","Xtr_list, ytr_list, gtr = build_candidates_and_labels(train_q)\n","Xdv_list, ydv_list, gdv = build_candidates_and_labels(dev_q)\n","len(Xtr_list), len(Xdv_list), sum(gtr), sum(gdv)"],"id":"AaqAXO983YUo"},{"cell_type":"markdown","metadata":{"id":"fMU3NwtB3YUp"},"source":["## 7) LightGBM LambdaMART Ranking"],"id":"fMU3NwtB3YUp"},{"cell_type":"code","metadata":{"id":"OYA20I5d3YUp"},"execution_count":null,"outputs":[],"source":["\n","def flatten_for_lgb(X_list, y_list):\n","    X = np.concatenate(X_list, axis=0)\n","    y = np.concatenate(y_list, axis=0)\n","    return X, y\n","\n","Xtr, ytr = flatten_for_lgb(Xtr_list, ytr_list)\n","Xdv, ydv = flatten_for_lgb(Xdv_list, ydv_list)\n","\n","lgb_train = lgb.Dataset(Xtr, label=ytr, group=gtr)\n","lgb_valid = lgb.Dataset(Xdv, label=ydv, group=gdv, reference=lgb_train)\n","\n","params = {\n","    \"objective\": \"lambdarank\",\n","    \"metric\": \"ndcg\",\n","    \"ndcg_at\": [10],\n","    \"learning_rate\": 0.05,\n","    \"num_leaves\": 63,\n","    \"min_data_in_leaf\": 50,\n","    \"feature_pre_filter\": False,\n","    \"verbose\": -1\n","}\n","\n","lgb_model = lgb.train(\n","    params,\n","    lgb_train,\n","    valid_sets=[lgb_valid],\n","    num_boost_round=300,\n","    early_stopping_rounds=30,\n","    verbose_eval=50\n",")"],"id":"OYA20I5d3YUp"},{"cell_type":"markdown","metadata":{"id":"W2H7MRu33YUp"},"source":["## 8) PyTorch LambdaRank (pairwise)"],"id":"W2H7MRu33YUp"},{"cell_type":"code","metadata":{"id":"XOO7V14h3YUp"},"execution_count":null,"outputs":[],"source":["\n","class LambdaRankTorch(nn.Module):\n","    def __init__(self, in_dim):\n","        super().__init__()\n","        self.mlp = nn.Sequential(\n","            nn.Linear(in_dim, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 1)\n","        )\n","    def forward(self, x):\n","        return self.mlp(x)\n","\n","def pairwise_loss(scores, labels):\n","    s = scores.view(-1)\n","    y = labels.view(-1)\n","    pos = torch.where(y > 0.5)[0]\n","    neg = torch.where(y < 0.5)[0]\n","    if len(pos) == 0 or len(neg) == 0:\n","        return None\n","    sp = s[pos].unsqueeze(1)\n","    sn = s[neg].unsqueeze(0)\n","    diff = sp - sn\n","    return torch.mean(torch.log1p(torch.exp(-diff)))\n","\n","def train_lambdarank_torch(X_list, y_list, epochs=3, lr=3e-3, device=DEVICE):\n","    model = LambdaRankTorch(in_dim=X_list[0].shape[1]).to(device)\n","    opt = optim.AdamW(model.parameters(), lr=lr)\n","    for ep in range(epochs):\n","        model.train()\n","        tot = 0.0; cnt = 0\n","        for Xq, yq in zip(X_list, y_list):\n","            xb = torch.tensor(Xq, dtype=torch.float32).to(device)\n","            yb = torch.tensor(yq, dtype=torch.float32).to(device)\n","            opt.zero_grad()\n","            sc = model(xb)\n","            loss = pairwise_loss(sc, yb)\n","            if loss is None: continue\n","            loss.backward(); opt.step()\n","            tot += loss.item(); cnt += 1\n","        print(f\"[Torch LambdaRank] epoch {ep} avg loss {tot/max(cnt,1):.4f}\")\n","    return model\n","\n","torch_model = train_lambdarank_torch(Xtr_list, ytr_list, epochs=3, lr=3e-3)"],"id":"XOO7V14h3YUp"},{"cell_type":"markdown","metadata":{"id":"srp5ixpN3YUq"},"source":["## 9) Evaluation: NDCG@10 / Recall@100 / Zero-Result Rate"],"id":"srp5ixpN3YUq"},{"cell_type":"code","metadata":{"id":"N_R3WjLU3YUq"},"execution_count":null,"outputs":[],"source":["\n","def ndcg_at_k(labels, scores, k=10):\n","    order = np.argsort(-scores)[:k]\n","    rel = np.array(labels)[order]\n","    gains = (2**rel - 1) / np.log2(np.arange(2, len(rel)+2))\n","    ideal = (2**sorted(labels, reverse=True) - 1) / np.log2(np.arange(2, len(rel)+2))\n","    ideal = ideal.sum() if len(ideal)>0 else 1.0\n","    return gains.sum()/ideal if ideal>0 else 0.0\n","\n","def eval_ranker_lightgbm(X_list, y_list, model):\n","    ndcgs, rec = [], []\n","    for Xq, yq in zip(X_list, y_list):\n","        sc = model.predict(Xq, num_iteration=model.best_iteration)\n","        ndcgs.append(ndcg_at_k(yq, sc, 10))\n","        top100 = np.argsort(-sc)[:100]\n","        rec.append(int(np.sum(np.array(yq)[top100])>0))\n","    return float(np.mean(ndcgs)), float(np.mean(rec))\n","\n","def eval_ranker_torch(X_list, y_list, model):\n","    model.eval()\n","    ndcgs, rec = [], []\n","    with torch.no_grad():\n","        for Xq, yq in zip(X_list, y_list):\n","            xb = torch.tensor(Xq, dtype=torch.float32).to(DEVICE)\n","            sc = model(xb).cpu().numpy().ravel()\n","            ndcgs.append(ndcg_at_k(yq, sc, 10))\n","            top100 = np.argsort(-sc)[:100]\n","            rec.append(int(np.sum(np.array(yq)[top100])>0))\n","    return float(np.mean(ndcgs)), float(np.mean(rec))\n","\n","lgb_ndcg10, lgb_recall100 = eval_ranker_lightgbm(Xdv_list, ydv_list, lgb_model)\n","torch_ndcg10, torch_recall100 = eval_ranker_torch(Xdv_list, ydv_list, torch_model)\n","\n","print(\"LightGBM   — NDCG@10:\", lgb_ndcg10,  \"Recall@100:\", lgb_recall100)\n","print(\"PyTorch    — NDCG@10:\", torch_ndcg10, \"Recall@100:\", torch_recall100)"],"id":"N_R3WjLU3YUq"},{"cell_type":"markdown","metadata":{"id":"-IXEikB63YUq"},"source":["## 10) Optional: Cross-Encoder Rerank (Budgeted Precision)"],"id":"-IXEikB63YUq"},{"cell_type":"code","metadata":{"id":"UyypvtPp3YUr"},"execution_count":null,"outputs":[],"source":["\n","USE_CE = bool(CONFIG.get(\"USE_CE_RERANK\", False))\n","if USE_CE:\n","    from sentence_transformers import CrossEncoder\n","    teacher_pairs = []\n","    for Xq, yq, qrow in zip(Xdv_list, ydv_list, dev_q.itertuples(index=False)):\n","        sc = lgb_model.predict(Xq, num_iteration=lgb_model.best_iteration)\n","        order = np.argsort(-sc)[:CONFIG[\"RERANK_TOPN\"]]\n","        for d in order:\n","            doc_text = docs_df.iloc[int(d)][\"doc_text\"]\n","            teacher_pairs.append([qrow.query, doc_text])\n","    ce = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\", device=DEVICE)\n","    ce_scores = ce.predict(teacher_pairs, batch_size=64, show_progress_bar=True)\n","    print(\"CE rerank scored pairs:\", len(ce_scores))\n","else:\n","    print(\"Skipping CE rerank. Set CONFIG['USE_CE_RERANK']=True to enable.\")"],"id":"UyypvtPp3YUr"},{"cell_type":"markdown","metadata":{"id":"STtDxI103YUr"},"source":["## 11) Latency Sampling (rough)"],"id":"STtDxI103YUr"},{"cell_type":"code","metadata":{"id":"XEWeuATY3YUr"},"execution_count":null,"outputs":[],"source":["\n","def time_block(fn, *args, repeats=5, **kwargs):\n","    times = []\n","    for _ in range(repeats):\n","        t0 = time.time()\n","        _ = fn(*args, **kwargs)\n","        times.append((time.time()-t0)*1000.0)\n","    return np.mean(times), np.percentile(times, 95)\n","\n","sample_qs = dev_q[\"query\"].head(64).tolist()\n","bm_t_mean, bm_t_p95 = time_block(lambda qs: bm25_search(qs, 50), sample_qs, repeats=3)\n","ann_t_mean, ann_t_p95 = time_block(lambda qs: ann_search(qs, 50), sample_qs, repeats=3)\n","print(f\"BM25 latency ~ mean {bm_t_mean:.1f}ms / p95 {bm_t_p95:.1f}ms (batch of {len(sample_qs)})\")\n","print(f\"ANN  latency ~ mean {ann_t_mean:.1f}ms / p95 {ann_t_p95:.1f}ms (batch of {len(sample_qs)})\")"],"id":"XEWeuATY3YUr"},{"cell_type":"markdown","metadata":{"id":"oYfAfESH3YUr"},"source":["## 12) Drift & Health Checks (PSI, Query Mix Entropy)"],"id":"oYfAfESH3YUr"},{"cell_type":"code","metadata":{"id":"ShVrDZd23YUr"},"execution_count":null,"outputs":[],"source":["\n","def psi(expected, actual, buckets=10, eps=1e-8):\n","    q = np.linspace(0, 100, buckets+1)\n","    cuts_e = np.percentile(expected, q)\n","    cuts_a = np.percentile(actual, q)\n","    cuts = np.unique(np.concatenate([cuts_e, cuts_a]))\n","    e_hist, _ = np.histogram(expected, bins=cuts)\n","    a_hist, _ = np.histogram(actual, bins=cuts)\n","    e = e_hist / (e_hist.sum() + eps)\n","    a = a_hist / (a_hist.sum() + eps)\n","    val = np.sum((a - e) * np.log((a + eps)/(e + eps)))\n","    return float(val)\n","\n","Xtr_all = np.concatenate(Xtr_list, axis=0)\n","Xdv_all = np.concatenate(Xdv_list, axis=0)\n","psis = [psi(Xtr_all[:,i], Xdv_all[:,i]) for i in range(Xtr_all.shape[1])]\n","for i,pv in enumerate(psis):\n","    print(f\"Feature {i} PSI: {pv:.4f}\")\n","\n","def q_entropy(qs):\n","    tokens = [str(q).split()[0].lower() if str(q).split() else \"<empty>\" for q in qs]\n","    vals, counts = np.unique(tokens, return_counts=True)\n","    p = counts / counts.sum()\n","    return float(entropy(p))\n","ent_train = q_entropy(train_q[\"query\"].tolist())\n","ent_dev = q_entropy(dev_q[\"query\"].tolist())\n","print(\"Query-mix entropy train/dev:\", ent_train, ent_dev)"],"id":"ShVrDZd23YUr"},{"cell_type":"markdown","metadata":{"id":"0C-tUWCk3YUr"},"source":["## 13) Interleaving Simulation (A vs B Rankers)"],"id":"0C-tUWCk3YUr"},{"cell_type":"code","metadata":{"id":"TofYog-03YUr"},"execution_count":null,"outputs":[],"source":["\n","def interleave(rankA, rankB, k=10):\n","    seen=set(); out=[]\n","    a=b=0\n","    while len(out)<k and (a<len(rankA) or b<len(rankB)):\n","        if len(out)%2==0:\n","            while a<len(rankA) and rankA[a] in seen: a+=1\n","            if a<len(rankA): out.append((rankA[a],\"A\")); seen.add(rankA[a]); a+=1\n","        else:\n","            while b<len(rankB) and rankB[b] in seen: b+=1\n","            if b<len(rankB): out.append((rankB[b],\"B\")); seen.add(rankB[b]); b+=1\n","    return out\n","\n","winsA=winsB=0; trials=500\n","for t in range(trials):\n","    A = list(np.random.permutation(200)[:20])\n","    B = list(np.random.permutation(200)[:20])\n","    il = interleave(A,B,10)\n","    relA = set(A[:10]); relB = set(B[:12])\n","    clicks = [1 if (d in relA or d in relB) and np.random.rand()<0.2 else 0 for d,_ in il]\n","    creditA = sum(c for (d,s),c in zip(il,clicks) if s==\"A\")\n","    creditB = sum(c for (d,s),c in zip(il,clicks) if s==\"B\")\n","    winsA += (creditA>creditB); winsB += (creditB>creditA)\n","print(\"Interleaving wins A:\", winsA, \"B:\", winsB)"],"id":"TofYog-03YUr"},{"cell_type":"markdown","metadata":{"id":"vX3JgFQv3YUs"},"source":["## 14) Vespa: Application Package (schema + feed + query) — *optional*"],"id":"vX3JgFQv3YUs"},{"cell_type":"markdown","metadata":{"id":"bktKcDXf3YUs"},"source":["> **Note:** Running Vespa locally on Colab is not supported (requires Docker / Java services).  \n","> This section *generates* a minimal Vespa **application package** (`schemas/`, `services.xml`, `hosts.xml`)\n","> and includes Python snippets to **deploy**, **feed documents**, and **query** when you run it on your own machine or Vespa Cloud."],"id":"bktKcDXf3YUs"},{"cell_type":"code","metadata":{"id":"-dWJfhTL3YUs"},"execution_count":null,"outputs":[],"source":["\n","import os, json, pathlib, textwrap\n","\n","VESPA_DIR = \"/content/vespa_app\"\n","os.makedirs(VESPA_DIR, exist_ok=True)\n","os.makedirs(os.path.join(VESPA_DIR, \"schemas\"), exist_ok=True)\n","\n","schema = r\"\"\"\n","schema product {\n","  document product {\n","    field product_id type string {\n","      indexing: attribute | summary\n","    }\n","    field doc_text type string {\n","      indexing: index | summary\n","      index: enable-bm25\n","    }\n","    field stars type double {\n","      indexing: attribute | summary\n","    }\n","  }\n","  fieldsets {\n","    default: doc_text\n","  }\n","  rank-profile bm25 {\n","    first-phase {\n","      expression: bm25(doc_text)\n","    }\n","  }\n","}\n","\"\"\"\n","\n","with open(os.path.join(VESPA_DIR, \"schemas\", \"product.sd\"), \"w\") as f:\n","    f.write(schema)\n","\n","services_xml = r\"\"\"\n","<services>\n","  <container id=\"default\" version=\"1.0\">\n","    <search/>\n","    <document-api/>\n","    <http>\n","      <server id=\"default\" port=\"8080\"/>\n","    </http>\n","    <component id=\"com.yahoo.language.simple.SimpleLinguistics\" bundle=\"language-tools\"/>\n","  </container>\n","  <content id=\"content\" version=\"1.0\">\n","    <documents>\n","      <document type=\"product\" mode=\"index\"/>\n","    </documents>\n","    <nodes count=\"1\"/>\n","  </content>\n","</services>\n","\"\"\"\n","\n","with open(os.path.join(VESPA_DIR, \"services.xml\"), \"w\") as f:\n","    f.write(services_xml)\n","\n","hosts_xml = \"<hosts><host name='localhost'><alias>node1</alias></host></hosts>\"\n","with open(os.path.join(VESPA_DIR, \"hosts.xml\"), \"w\") as f:\n","    f.write(hosts_xml)\n","\n","print(\"Vespa app package written to:\", VESPA_DIR)\n","print(\"Next steps (run locally with Docker):\")\n","print(\"1) docker run --detach --name vespa --hostname vespa-container --privileged -p 8080:8080 -p 19071:19071 vespaengine/vespa\")\n","print(\"2) vespa deploy --wait 300\", VESPA_DIR)\n","print(\"3) Feed via /document/v1/, query via /search/ yql='select * from sources product where userInput(@q)'\")"],"id":"-dWJfhTL3YUs"},{"cell_type":"markdown","metadata":{"id":"M9QgD9bq3YUs"},"source":["### Vespa feed/query helpers (execute when Vespa endpoint is reachable)"],"id":"M9QgD9bq3YUs"},{"cell_type":"code","metadata":{"id":"ZPxH1iUb3YUs"},"execution_count":null,"outputs":[],"source":["\n","import requests\n","\n","VESPA_ENDPOINT = None  # e.g., \"http://localhost:8080\"\n","\n","def vespa_feed_docs(endpoint, docs):\n","    for d in docs:\n","        url = f\"{endpoint}/document/v1/product/product/docid/{d['product_id']}\"\n","        r = requests.post(url, json={\"fields\": d})\n","        if r.status_code//100 != 2:\n","            print(\"Feed error:\", r.status_code, r.text)\n","            break\n","\n","def vespa_query(endpoint, q, hits=10):\n","    params = {\"yql\": \"select * from sources product where userInput(@q)\", \"q\": q, \"hits\": hits}\n","    r = requests.get(f\"{endpoint}/search/\", params=params)\n","    return r.json()\n","\n","# Example usage (uncomment when endpoint is set):\n","# if VESPA_ENDPOINT:\n","#     feed = docs_df.head(1000).rename(columns={\"doc_text\":\"doc_text\",\"stars\":\"stars\"}).to_dict(orient=\"records\")\n","#     vespa_feed_docs(VESPA_ENDPOINT, feed)\n","#     print(vespa_query(VESPA_ENDPOINT, \"iphone case\"))"],"id":"ZPxH1iUb3YUs"},{"cell_type":"markdown","metadata":{"id":"lfuAsT6M3YUt"},"source":["## 15) Wrap-up & Next Steps"],"id":"lfuAsT6M3YUt"},{"cell_type":"markdown","metadata":{"id":"oKwdCJQH3YUt"},"source":["You now have a working **retrieval + ranking** stack with:\n","- **FAISS HNSW** dense retrieval + BM25 lexical, fused by RRF\n","- **LightGBM LambdaMART** vs **PyTorch LambdaRank** comparison\n","- Optional **cross-encoder** rerank\n","- Basic **latency**, **drift**, and **interleaving** utilities\n","- A ready-to-deploy **Vespa** application package scaffold (run locally or in Vespa Cloud)\n","\n","**Extensions**\n","- Add category/brand/entity KG features, seller quality, price/discount, ETA.\n","- Swap to **BEIR**/MS MARCO for broader retrieval eval.\n","- Add a simple **FastAPI** microservice for online scoring."],"id":"oKwdCJQH3YUt"}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10"}},"nbformat":4,"nbformat_minor":5}