{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# 1) Hybrid Retrieval: BM25 + FAISS + Torch Ranker (<100ms-style)"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "\n%%capture\n!pip -q install --upgrade pip\n!pip -q install datasets transformers sentence-transformers faiss-cpu rank-bm25 torchmetrics scikit-learn lightgbm langdetect unidecode pandas matplotlib tqdm nltk"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "\nimport os, numpy as np, pandas as pd, torch, time\nfrom datasets import load_dataset\nfrom sentence_transformers import SentenceTransformer\nfrom rank_bm25 import BM25Okapi\nimport faiss\nfrom sklearn.model_selection import train_test_split\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm.auto import tqdm\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nSEED = 42; np.random.seed(SEED); torch.manual_seed(SEED)\nCONFIG = {\"language\":\"en\",\"N_DOCS\":30000,\"N_QUERIES\":3000,\"TOPK_BM25\":200,\"TOPK_ANN\":200,\"FUSION_K\":300}"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "\ndef load_amz(lang=\"en\", n_docs=30000, n_queries=3000):\n    ds = load_dataset(\"amazon_reviews_multi\", lang, split=\"train\")\n    df = ds.to_pandas()[[\"product_id\",\"review_title\",\"review_body\"]].dropna()\n    g = df.groupby(\"product_id\")\n    prod = g.agg({\"review_title\":lambda s:\" | \".join(s.head(10).astype(str)),\n                  \"review_body\":lambda s:\" \".join(s.head(5).astype(str))}).reset_index()\n    prod[\"doc_text\"] = (prod[\"review_title\"].fillna(\"\")+\" \"+prod[\"review_body\"].fillna(\"\")).str.strip()\n    prod = prod[prod[\"doc_text\"].str.len()>16].sample(frac=1, random_state=SEED).head(n_docs).reset_index(drop=True)\n    pids = set(prod[\"product_id\"])\n    q = df[df[\"product_id\"].isin(pids)][[\"review_title\",\"product_id\"]].dropna()\n    q = q.rename(columns={\"review_title\":\"query\",\"product_id\":\"relevant_pid\"}).drop_duplicates().sample(frac=1, random_state=SEED).head(n_queries).reset_index(drop=True)\n    return prod[[\"product_id\",\"doc_text\"]], q\ndocs_df, queries_df = load_amz(CONFIG[\"language\"], CONFIG[\"N_DOCS\"], CONFIG[\"N_QUERIES\"])"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "\ndef tok(txt): \n    txt = str(txt).lower().replace(\"\\n\",\" \").strip()\n    return [t for t in txt.split() if t]\ncorpus_tok = [tok(t) for t in docs_df[\"doc_text\"].tolist()]\nbm25 = BM25Okapi(corpus_tok)\ndense = SentenceTransformer(\"intfloat/multilingual-e5-base\", device=device)"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "\ndef encode(texts, bs=128):\n    vecs = []\n    for i in range(0, len(texts), bs):\n        emb = dense.encode(texts[i:i+bs], batch_size=bs, normalize_embeddings=True, convert_to_numpy=True, show_progress_bar=False)\n        vecs.append(emb)\n    return np.vstack(vecs).astype(\"float32\")\ndoc_emb = encode(docs_df[\"doc_text\"].tolist(), 128)\nindex = faiss.IndexHNSWFlat(doc_emb.shape[1], 32); index.hnsw.efConstruction = 200; index.hnsw.efSearch = 128\nindex.add(doc_emb)"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "\ndef bm25_search(qs, k):\n    out=[]\n    for q in qs:\n        s = bm25.get_scores(tok(q))\n        top = np.argpartition(s, -k)[-k:]\n        top = top[np.argsort(-s[top])]\n        out.append((top, s[top]))\n    return out\ndef ann_search(qs, k):\n    qv = encode(qs, 128)\n    sco, idx = index.search(qv, k)\n    return idx, sco\ndef rrf(bm_idx, ann_idx, k=300, K=60):\n    fused=[]\n    for qi in range(len(bm_idx)):\n        ranks={}\n        for r,d in enumerate(bm_idx[qi]): ranks[d]=ranks.get(d,0)+1.0/(K+r+1)\n        for r,d in enumerate(ann_idx[qi]): ranks[d]=ranks.get(d,0)+1.0/(K+r+1)\n        items=sorted(ranks.items(), key=lambda x:-x[1])[:k]\n        fused.append(np.array([i for i,_ in items], dtype=int))\n    return fused"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "\ntrain_q, dev_q = train_test_split(queries_df, test_size=0.2, random_state=42)\ndef make_split(qdf):\n    qs = qdf[\"query\"].tolist()\n    bm = bm25_search(qs, CONFIG[\"TOPK_BM25\"]); bm_idx=[o[0] for o in bm]\n    ann_idx, _ = ann_search(qs, CONFIG[\"TOPK_ANN\"])\n    fused = rrf(bm_idx, ann_idx, CONFIG[\"FUSION_K\"])\n    X, y = [], []\n    for i,q in enumerate(qdf.itertuples()):\n        feats=[]; labs=[]\n        for d in fused[i]:\n            bmr = np.where(bm_idx[i]==d)[0][0] if d in bm_idx[i] else 9999\n            anr = np.where(ann_idx[i]==d)[0][0] if d in ann_idx[i] else 9999\n            feats.append([bmr, anr])\n            labs.append(1 if docs_df.iloc[d][\"product_id\"]==q.relevant_pid else 0)\n        if sum(labs)==0: continue\n        X.extend(feats); y.extend(labs)\n    import torch\n    X = torch.tensor(np.array(X), dtype=torch.float32); y = torch.tensor(np.array(y), dtype=torch.float32).unsqueeze(1)\n    return X, y\nXtr, ytr = make_split(train_q); Xdv, ydv = make_split(dev_q)"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "\nimport torch.nn as nn, torch.optim as optim, torch\nclass MLP(nn.Module):\n    def __init__(self): super().__init__(); self.net=nn.Sequential(nn.Linear(2,32),nn.ReLU(),nn.Linear(32,1))\n    def forward(self,x): return self.net(x)\nmodel=MLP().to(device); opt=optim.AdamW(model.parameters(), lr=3e-3); loss=nn.BCEWithLogitsLoss()\nfor ep in range(3):\n    model.train(); tot=0\n    for i in range(0,len(Xtr),512):\n        xb, yb=Xtr[i:i+512].to(device), ytr[i:i+512].to(device)\n        opt.zero_grad(); p=model(xb); l=loss(p,yb); l.backward(); opt.step(); tot+=l.item()*len(xb)\n    print(\"ep\", ep, \"loss\", tot/len(Xtr))"}], "metadata": {"colab": {"provenance": []}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "nbformat": 4, "nbformat_minor": 5}