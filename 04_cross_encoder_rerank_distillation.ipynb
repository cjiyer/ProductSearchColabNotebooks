{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# 4) Cross-Encoder Reranking + Distillation to Dual-Encoder"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "\n%%capture\n!pip -q install --upgrade pip\n!pip -q install datasets transformers sentence-transformers faiss-cpu rank-bm25 torchmetrics scikit-learn lightgbm langdetect unidecode pandas matplotlib tqdm nltk"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "\nimport numpy as np, torch, faiss\nfrom datasets import load_dataset\nfrom sentence_transformers import SentenceTransformer, CrossEncoder, InputExample, losses\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\""}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "\ntrain = load_dataset(\"ms_marco\",\"v2.1\", split=\"train[:1%]\")\npairs = []\nfor r in train:\n    q = r[\"query\"]\n    doc = r[\"wellFormedAnswers\"][0] if r[\"wellFormedAnswers\"] else (r[\"passages\"][\"passage_text\"][0] if r[\"passages\"][\"passage_text\"] else None)\n    if doc: pairs.append((q, doc))\ncorpus = [d for (_,d) in pairs[:30000]]\nqueries = [q for (q,_) in pairs[:2000]]"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "\nstudent = SentenceTransformer(\"sentence-transformers/msmarco-distilbert-base-tas-b\", device=device)\ndoc_vec = student.encode(corpus, batch_size=128, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=True).astype(\"float32\")\nq_vec = student.encode(queries, batch_size=128, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=True).astype(\"float32\")\nindex = faiss.IndexFlatIP(doc_vec.shape[1]); index.add(doc_vec)\nscores, idx = index.search(q_vec, 50)"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "\nce = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\", device=device)\nteacher_pairs = []\nfor i,q in enumerate(queries):\n    for j in idx[i]:\n        teacher_pairs.append([q, corpus[j]])\nteacher_scores = ce.predict(teacher_pairs, batch_size=64, show_progress_bar=True)"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "\ntrain_data = [InputExample(texts=p, label=float(s)) for p,s in zip(teacher_pairs, teacher_scores)]\nloader = DataLoader(train_data, batch_size=64, shuffle=True)\nloss = losses.CosineSimilarityLoss(student)\nstudent.fit([(loader, loss)], epochs=1, warmup_steps=50, output_path=\"artifacts_ce_distilled_student\")\nprint(\"Distillation complete. Saved model.\")"}], "metadata": {"colab": {"provenance": []}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "nbformat": 4, "nbformat_minor": 5}