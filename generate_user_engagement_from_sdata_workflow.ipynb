{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth application-default login --no-browser"
      ],
      "metadata": {
        "id": "gJbIh6wBzQUA"
      },
      "id": "gJbIh6wBzQUA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "1RMX2DQNHlDiktwKdG2bKD9Q",
      "metadata": {
        "tags": [],
        "id": "1RMX2DQNHlDiktwKdG2bKD9Q"
      },
      "source": [
        "# Import required libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "import logging\n",
        "import os\n",
        "from datetime import datetime, timedelta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "# Clear any existing GPU memory\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Set PyTorch memory allocation strategy\n",
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
      ],
      "metadata": {
        "id": "aIZLpDDEiR0z"
      },
      "id": "aIZLpDDEiR0z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "\n",
        "# credentials_path = '/content/.config/application_default_credentials.json'\n",
        "# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credentials_path\n",
        "# Set Java home explicitly\n",
        "# os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-17-openjdk-amd64'  # Adjust path as needed\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"generate_ext_user_engagement_dataset\") \\\n",
        "    .config(\n",
        "        \"spark.jars.packages\",\n",
        "        \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.13:0.43.1,\"\n",
        "        \"com.google.cloud.bigdataoss:gcs-connector:hadoop3-2.2.30\"\n",
        "    ) \\\n",
        "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
        "    .config(\"spark.hadoop.google.cloud.auth.application_default_credentials.enabled\", \"true\") \\\n",
        "    .config(\"spark.driver.memory\", \"100g\") \\\n",
        "    .config(\"spark.driver.memoryOverhead\", \"20g\") \\\n",
        "    .config(\"spark.executor.memory\", \"100g\") \\\n",
        "    .config(\"spark.executor.memoryOverhead\", \"20g\") \\\n",
        "    .config(\"spark.executor.cores\", \"12\") \\\n",
        "    .config(\"spark.executor.instances\", \"2\") \\\n",
        "    .config(\"spark.driver.maxResultSize\", \"20g\") \\\n",
        "    .config(\"spark.default.parallelism\", \"96\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"96\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \\\n",
        "    .config(\"spark.local.dir\", \"/content/spark-temp\") \\\n",
        "    .config(\"hadoop.tmp.dir\", \"/content/hadoop-tmp\") \\\n",
        "    .config(\"spark.sql.warehouse.dir\", \"/content/spark-warehouse\") \\\n",
        "    .config(\"spark.kryoserializer.buffer.max\", \"1024m\") \\\n",
        "    .config(\"spark.driver.extraJavaOptions\", \"-Djava.io.tmpdir=/content/tmp\") \\\n",
        "    .config(\"spark.executor.extraJavaOptions\", \"-Djava.io.tmpdir=/content/tmp\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Spark Session initialized successfully!\")\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "\n",
        "# Configuration\n",
        "PROJECT_ID = \"honey-production\"  # Replace with your GCP project ID\n",
        "DATASET = \"sdata_events_partitioned\"       # Replace with your dataset name\n",
        "\n",
        "# Date parameters (modify as needed)\n",
        "ANALYSIS_START_DATE = \"2025-12-08\"\n",
        "ANALYSIS_END_DATE = \"2025-12-08\"\n",
        "\n",
        "print(f\"Querying data from {ANALYSIS_START_DATE} to {ANALYSIS_END_DATE}\")"
      ],
      "metadata": {
        "id": "T6gGYMUT0xTJ"
      },
      "id": "T6gGYMUT0xTJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create temp directories on local SSD\n",
        "import os\n",
        "os.makedirs(\"/content/spark-temp\", exist_ok=True)\n",
        "os.makedirs(\"/content/spark-warehouse\", exist_ok=True)\n",
        "os.makedirs(\"/content/hadoop-tmp\", exist_ok=True)"
      ],
      "metadata": {
        "id": "dzUlOFbORdBe"
      },
      "id": "dzUlOFbORdBe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page_detected_sql_query = f\"\"\"\n",
        "SELECT distinct\n",
        "    id,\n",
        "    ts,\n",
        "    timestamp,\n",
        "    store.id AS storeid,\n",
        "    store.name AS storename,\n",
        "    store.country AS storecountry,\n",
        "    store.session_id AS storesessionid,\n",
        "    extension.screenview_id AS screenviewid,\n",
        "    page_title,\n",
        "    page_type,\n",
        "    user_id,\n",
        "    session_id,\n",
        "    referrer_url\n",
        "FROM `{PROJECT_ID}.{DATASET}.ext_page_detected`\n",
        "WHERE timestamp IS NOT NULL\n",
        "AND DATE(timestamp) >= '{ANALYSIS_START_DATE}'\n",
        "AND DATE(timestamp) <= '{ANALYSIS_END_DATE}'\n",
        "ORDER BY session_id DESC, user_id DESC, ts DESC\n",
        "\"\"\"\n",
        "# Execute query and create DataFrame\n",
        "print(\"Executing BigQuery SQL...\")\n",
        "page_detected_df = spark.read \\\n",
        "    .format(\"bigquery\") \\\n",
        "    .option(\"query\", page_detected_sql_query) \\\n",
        "    .option(\"project\", PROJECT_ID) \\\n",
        "    .option(\"viewsEnabled\", \"true\") \\\n",
        "    .load()\n",
        "    # .option(\"materializationProject\", PROJECT_ID) \\\n",
        "    # .option(\"materializationDataset\", \"new_temp_materialization\") \\\n",
        "    # .option(\"dataset\", \"US\") \\\n",
        "    # .load()\n",
        "# print(f\"\\n=== SAMPLE DATA (First 50 rows) ===\")\n",
        "# page_detected_df.show(50, truncate=False)"
      ],
      "metadata": {
        "id": "1aiqSeVN01mD"
      },
      "id": "1aiqSeVN01mD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def extract_shein_sc_query_term(search_string):\n",
        "    \"\"\"\n",
        "    Extract the query term from sc=<queryterm> in the search string.\n",
        "    \"\"\"\n",
        "    if not search_string:\n",
        "        return None\n",
        "\n",
        "    # Pattern: sc= followed by anything until the next backtick or end of string\n",
        "    match = re.search(r'sc=([^`]+)', search_string)\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    return None"
      ],
      "metadata": {
        "id": "sHf3mxCzUUnE"
      },
      "id": "sHf3mxCzUUnE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.parse import urlparse, parse_qs\n",
        "import pandas as pd\n",
        "\n",
        "def extract_search_query(referrer_url):\n",
        "    \"\"\"\n",
        "    Extract the search query term from a referrer URL based on known key variants for different merchants.\n",
        "    :param referrer_url: Referrer URL string.\n",
        "    :return: Extracted search query term or None if no match is found.\n",
        "    \"\"\"\n",
        "    # Ensure referrer_url is a string; handle NaN values which are floats\n",
        "    if pd.isna(referrer_url):\n",
        "        return None\n",
        "\n",
        "    # List of key variants used by merchants to denote search query terms\n",
        "    search_query_keys = [\n",
        "        'keywords',                # Amazon\n",
        "        '_skw',                    # eBay\n",
        "        'src_identifier',          # Shein\n",
        "        'ga_search_query',         # Etsy\n",
        "        'search_key',              # Temu\n",
        "        'searchTerm',              # Lowe's\n",
        "        'searchText',              # Old Navy\n",
        "        'q',                       # Etsy, Walmart, REI\n",
        "        'kw',                      # StubHub\n",
        "        'keyword',                 # Costco, eBay\n",
        "        'search'                  # BestBuy, Menards, ULTA Beauty\n",
        "    ]\n",
        "\n",
        "    # Parse the referrer URL to extract the query string parameters\n",
        "    parsed_url = urlparse(referrer_url)\n",
        "    query_params = parse_qs(parsed_url.query)\n",
        "\n",
        "    # Check for search query keys in the query parameters\n",
        "    for key in search_query_keys:\n",
        "        if key in query_params:\n",
        "            # Return the first matching value for the key\n",
        "            sanitized_query_term = query_params[key][0]\n",
        "            if sanitized_query_term:\n",
        "                sanitized_query_term = sanitized_query_term.strip()\n",
        "                if sanitized_query_term.startswith('\"') and sanitized_query_term.endswith('\"'):\n",
        "                    sanitized_query_term = sanitized_query_term[1:-1]\n",
        "                sanitized_query_term = sanitized_query_term.replace('+', ' ')\n",
        "                sanitized_query_term = re.sub(r'\\s+', ' ', sanitized_query_term)\n",
        "                sanitized_query_term = sanitized_query_term.lower()\n",
        "                sanitized_query_term = sanitized_query_term.strip()\n",
        "                #Do a SHEIN specific Search query term extraction\n",
        "                if key == 'src_identifier':\n",
        "                    if 'src_module' in query_params and query_params['src_module'][0] == 'search':\n",
        "                        sanitized_query_term = extract_shein_sc_query_term(sanitized_query_term)\n",
        "                    else:\n",
        "                        sanitized_query_term = None\n",
        "            return sanitized_query_term\n",
        "            # return query_params[key][0]  # Assuming the first value is the desired search term\n",
        "\n",
        "    # If no matching key is found, return None\n",
        "    return None"
      ],
      "metadata": {
        "id": "BfoNZVmBUIaT"
      },
      "id": "BfoNZVmBUIaT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Filter for specific page_types\n",
        "allowed_page_types = ['UNSUPPORTED', 'PRODUCT', 'SEARCH', 'CART_PRODUCT', 'CHECKOUT_CONFIRM']\n",
        "ATC_CHECKOUT_PAGE_TYPE = ['CART_PRODUCT', 'CHECKOUT_CONFIRM']\n",
        "page_detected_df_filtered = page_detected_df.filter(F.col('page_type').isin(allowed_page_types))\n",
        "\n",
        "# Register UDF\n",
        "extract_search_query_udf = F.udf(extract_search_query, StringType())\n",
        "\n",
        "# Apply transformations\n",
        "page_detected_df_final = page_detected_df_filtered.withColumn('search_query_term', extract_search_query_udf(F.col('referrer_url')))\n",
        "\n",
        "# # Show results\n",
        "# page_detected_df_final.show(50, truncate=False)\n",
        "# print(f\"Total records after filtering: {page_detected_df_final.count()}\")\n",
        "\n",
        "# Filter rows with non-null search_query_term\n",
        "filtered_click_df = page_detected_df_final.filter(F.col('search_query_term').isNotNull())\n",
        "\n",
        "# Get unique user_id, session_id combinations from filtered_click_df\n",
        "filtered_click_df_merge_fields = filtered_click_df.select('user_id', 'session_id').distinct()\n",
        "\n",
        "# Filter rows with page_type in ATC_CHECKOUT_PAGE_TYPE\n",
        "atc_checkout_df = page_detected_df_final.filter(F.col('page_type').isin(ATC_CHECKOUT_PAGE_TYPE))\n",
        "\n",
        "# Filter rows where search_query_term is null and page_type is 'PRODUCT'\n",
        "product_non_search_query_terms_df = page_detected_df_final.filter(\n",
        "    F.col('search_query_term').isNull() &\n",
        "    (F.col('page_type') == 'PRODUCT')\n",
        ")\n",
        "\n",
        "# Union the two dataframes\n",
        "atc_checkout_df = atc_checkout_df.union(product_non_search_query_terms_df)\n",
        "\n",
        "# Inner join and select specific columns\n",
        "filtered_atc_checkout_df = atc_checkout_df.join(\n",
        "    filtered_click_df_merge_fields,\n",
        "    on=['user_id', 'session_id'],\n",
        "    how='inner'\n",
        ").select(\n",
        "    'id', 'ts', 'timestamp', 'storeid', 'storename', 'storecountry',\n",
        "    'storesessionid', 'screenviewid', 'page_title', 'page_type', 'user_id',\n",
        "    'session_id', 'referrer_url', 'search_query_term'\n",
        ").distinct()\n",
        "\n",
        "# Final union\n",
        "filtered_click_atc_checkout_df = filtered_click_df.union(filtered_atc_checkout_df)\n",
        "filtered_click_atc_checkout_df = filtered_click_atc_checkout_df.withColumn(\n",
        "    \"is_engagement_verified\",\n",
        "    F.when(F.col(\"search_query_term\").isNotNull(), 1).otherwise(0)\n",
        ")\n",
        "\n",
        "# filtered_click_atc_checkout_df.show(100, truncate=False)\n",
        "# print(f\"Total records after filtering for user sessions with at least one search query term: {filtered_click_atc_checkout_df.count()}\")"
      ],
      "metadata": {
        "id": "zVGsYLSu-0aA"
      },
      "id": "zVGsYLSu-0aA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "atc_products_sql_query = f\"\"\"\n",
        "SELECT distinct\n",
        "extension.screenview_id AS screenview_id,\n",
        "product.parent_id AS parentid, product.name AS productname, product.url AS producturl, product.total_price AS totalprice, product.sku AS productsku,\n",
        "user_id AS userid, session_id AS sessionid\n",
        "FROM `{PROJECT_ID}.{DATASET}.ext_cart_products`, UNNEST(cart.products) AS product\n",
        "WHERE timestamp IS NOT NULL\n",
        "AND DATE(timestamp) >= '{ANALYSIS_START_DATE}'\n",
        "AND DATE(timestamp) <= '{ANALYSIS_END_DATE}'\n",
        "ORDER BY session_id DESC, user_id DESC\n",
        "\"\"\"\n",
        "# Execute query and create DataFrame\n",
        "print(\"Executing BigQuery SQL...\")\n",
        "atc_products_df = spark.read \\\n",
        "    .format(\"bigquery\") \\\n",
        "    .option(\"query\", atc_products_sql_query) \\\n",
        "    .option(\"project\", PROJECT_ID) \\\n",
        "    .option(\"viewsEnabled\", \"true\") \\\n",
        "    .load()\n",
        "\n",
        "# print(f\"\\n=== SAMPLE DATA (First 20 rows) ===\")\n",
        "# atc_products_df.show(50, truncate=False)\n",
        "\n",
        "# Define the join condition\n",
        "join_condition = [\n",
        "    filtered_click_atc_checkout_df.user_id == atc_products_df.userid,\n",
        "    filtered_click_atc_checkout_df.session_id == atc_products_df.sessionid,\n",
        "    filtered_click_atc_checkout_df.screenviewid == atc_products_df.screenview_id\n",
        "]\n",
        "\n",
        "# Perform the join\n",
        "joined_df = filtered_click_atc_checkout_df.join(F.broadcast(atc_products_df), on=join_condition, how='left')\n",
        "\n",
        "# Update the page_title field and add is_engagement_verified\n",
        "result_df = joined_df.withColumn(\n",
        "    \"page_title\",\n",
        "    F.when(F.col(\"productname\").isNotNull(), F.col(\"productname\")).otherwise(F.col(\"page_title\"))\n",
        ").withColumn(\n",
        "    \"is_engagement_verified\",\n",
        "    F.when(F.col(\"productname\").isNotNull(), 1).otherwise(F.col(\"is_engagement_verified\"))\n",
        ")\n",
        "\n",
        "# Select fields only from `filtered_click_atc_checkout_df`\n",
        "engagement_click_atc_df_filtered = result_df.select(*[F.col(field) for field in filtered_click_atc_checkout_df.columns])\n",
        "\n",
        "# Show the final dataframe\n",
        "engagement_click_atc_df_filtered.show(100, truncate=False)"
      ],
      "metadata": {
        "id": "JsHsmup84nSD"
      },
      "id": "JsHsmup84nSD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkout_products_sql_query = f\"\"\"\n",
        "SELECT distinct\n",
        "extension.screenview_id AS screenview_id,\n",
        "product.parent_id AS parentid, product.name AS productname, product.url AS producturl, product.total_price AS totalprice, product.sku AS productsku,\n",
        "user_id AS userid, session_id AS sessionid\n",
        "FROM `{PROJECT_ID}.{DATASET}.ext_checkout_products`, UNNEST(cart.products) AS product\n",
        "WHERE timestamp IS NOT NULL\n",
        "AND DATE(timestamp) >= '{ANALYSIS_START_DATE}'\n",
        "AND DATE(timestamp) <= '{ANALYSIS_END_DATE}'\n",
        "ORDER BY session_id DESC, user_id DESC\n",
        "\"\"\"\n",
        "# Execute query and create DataFrame\n",
        "print(\"Executing BigQuery SQL...\")\n",
        "checkout_products_df = spark.read \\\n",
        "    .format(\"bigquery\") \\\n",
        "    .option(\"query\", checkout_products_sql_query) \\\n",
        "    .option(\"project\", PROJECT_ID) \\\n",
        "    .option(\"viewsEnabled\", \"true\") \\\n",
        "    .load()\n",
        "\n",
        "# print(f\"\\n=== SAMPLE DATA (First 50 rows) ===\")\n",
        "# checkout_products_df.show(50, truncate=False)\n",
        "# Define the join condition\n",
        "join_condition = [\n",
        "    engagement_click_atc_df_filtered.user_id == checkout_products_df.userid,\n",
        "    engagement_click_atc_df_filtered.session_id == checkout_products_df.sessionid,\n",
        "    engagement_click_atc_df_filtered.screenviewid == checkout_products_df.screenview_id\n",
        "]\n",
        "\n",
        "# Perform the join\n",
        "joined_df = engagement_click_atc_df_filtered.join(F.broadcast(checkout_products_df), on=join_condition, how='left')\n",
        "\n",
        "# Update the page_title field and add is_engagement_verified\n",
        "result_df = joined_df.withColumn(\n",
        "    \"page_title\",\n",
        "    F.when(F.col(\"productname\").isNotNull(), F.col(\"productname\")).otherwise(F.col(\"page_title\"))\n",
        ").withColumn(\n",
        "    \"is_engagement_verified\",\n",
        "    F.when(F.col(\"productname\").isNotNull(), 1).otherwise(F.col(\"is_engagement_verified\"))\n",
        ")\n",
        "\n",
        "# Select fields only from `filtered_click_atc_checkout_df`\n",
        "all_engagement_filtered_df = result_df.select(*[F.col(field) for field in engagement_click_atc_df_filtered.columns])\n",
        "all_engagement_filtered_ordered_df = all_engagement_filtered_df.orderBy(['session_id', 'user_id', 'ts'], ascending=False)\n",
        "all_engagement_filtered_ordered_df_with_date = all_engagement_filtered_ordered_df.withColumn(\"date\", F.to_date(all_engagement_filtered_ordered_df[\"timestamp\"]))\n",
        "# Show the final dataframe\n",
        "all_engagement_filtered_ordered_df.show(100, truncate=False)"
      ],
      "metadata": {
        "id": "ttLc75bP6YgH"
      },
      "id": "ttLc75bP6YgH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # GCS bucket path\n",
        "# gcs_path = \"gs://chanderiyer/projects/honeysearchbenchmarks/datasets/user_engagement/sdata_with_search_query_extract\"\n",
        "\n",
        "# # Loop through each partition and coalesce into a single file for each date subfolder\n",
        "# for date in all_engagement_filtered_ordered_df_with_date.select(\"date\").distinct().collect():\n",
        "#     single_date = date[\"date\"]\n",
        "#     # Filter by the specific date\n",
        "#     filtered_df = all_engagement_filtered_ordered_df_with_date.filter(F.col(\"date\") == single_date)\n",
        "#     # Coalesce the DataFrame to ensure a single file per date subfolder\n",
        "#     filtered_df.coalesce(1).write \\\n",
        "#         .option(\"header\", \"true\") \\\n",
        "#         .option(\"sep\", \"\\t\") \\\n",
        "#         .mode(\"overwrite\") \\\n",
        "#         .csv(f\"{gcs_path}/date={single_date}\")\n",
        "\n",
        "# print(f\"Data successfully written to {gcs_path} with coalesced files in each date subfolder.\")"
      ],
      "metadata": {
        "id": "uoWLpNndjbZJ"
      },
      "id": "uoWLpNndjbZJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Window\n",
        "from pyspark.sql.functions import col, when, last, sum as spark_sum, lag, coalesce\n",
        "\n",
        "# Define window spec partitioned by user_id and session_id, ordered by timestamp\n",
        "window_spec = Window.partitionBy(\"user_id\", \"session_id\").orderBy(\"ts\")\n",
        "\n",
        "# Step 1: Identify source rows (where search_query_term can be propagated FROM)\n",
        "df = all_engagement_filtered_ordered_df_with_date.withColumn(\n",
        "    \"is_source\",\n",
        "    when(\n",
        "        (col(\"search_query_term\").isNotNull()) &\n",
        "        (col(\"page_type\").isin(['PRODUCT', 'UNSUPPORTED', 'SEARCH'])),\n",
        "        1\n",
        "    ).otherwise(0)\n",
        ")\n",
        "\n",
        "# Check if CHECKOUT_CONFIRM exists in the session\n",
        "df = df.withColumn(\n",
        "    \"has_checkout_confirm\",\n",
        "    spark_sum(when(col(\"page_type\") == \"CHECKOUT_CONFIRM\", 1).otherwise(0)).over(\n",
        "        Window.partitionBy(\"user_id\", \"session_id\")\n",
        "    )\n",
        ")\n",
        "\n",
        "# Create stop event based on priority\n",
        "df = df.withColumn(\n",
        "    \"is_stop_event\",\n",
        "    when(\n",
        "        col(\"has_checkout_confirm\") > 0,\n",
        "        when(col(\"page_type\") == \"CHECKOUT_CONFIRM\", 1).otherwise(0)\n",
        "    ).otherwise(\n",
        "        when(col(\"page_type\") == \"CART_PRODUCT\", 1).otherwise(0)\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "# Lag the stop event to mark the row AFTER as a boundary\n",
        "df = df.withColumn(\n",
        "    \"prev_is_stop_event\",\n",
        "    lag(\"is_stop_event\", 1, 0).over(window_spec)\n",
        ")\n",
        "\n",
        "# Step 3: Create segment_id by cumulative sum of prev_is_stop_event\n",
        "df = df.withColumn(\n",
        "    \"segment_id\",\n",
        "    spark_sum(\"prev_is_stop_event\").over(window_spec.rowsBetween(Window.unboundedPreceding, Window.currentRow))\n",
        ")\n",
        "\n",
        "# Step 4: Forward fill search_query_term within each segment\n",
        "window_spec_segment = Window.partitionBy(\"user_id\", \"session_id\", \"segment_id\").orderBy(\"ts\")\n",
        "\n",
        "df = df.withColumn(\n",
        "    \"propagated_search_query_term\",\n",
        "    last(when(col(\"is_source\") == 1, col(\"search_query_term\")), ignorenulls=True)\n",
        "        .over(window_spec_segment.rowsBetween(Window.unboundedPreceding, Window.currentRow))\n",
        ")\n",
        "\n",
        "# Step 5: Apply derivation logic only to target page_types\n",
        "df = df.withColumn(\n",
        "    \"is_search_query_term_derived\",\n",
        "    when(\n",
        "        (col(\"page_type\").isin(['PRODUCT', 'UNSUPPORTED', 'CART_PRODUCT', 'CHECKOUT_CONFIRM'])) &\n",
        "        (col(\"search_query_term\").isNull()) &\n",
        "        (col(\"propagated_search_query_term\").isNotNull()),\n",
        "        1\n",
        "    ).otherwise(0)\n",
        ")\n",
        "\n",
        "# Step 6: Update search_query_term with propagated value where derived\n",
        "df = df.withColumn(\n",
        "    \"search_query_term\",\n",
        "    when(\n",
        "        col(\"is_search_query_term_derived\") == 1,\n",
        "        col(\"propagated_search_query_term\")\n",
        "    ).otherwise(col(\"search_query_term\"))\n",
        ")\n",
        "\n",
        "# Step 7: Clean up helper columns\n",
        "df = df.drop(\"is_source\", \"has_checkout_confirm\", \"is_stop_event\", \"prev_is_stop_event\", \"segment_id\", \"propagated_search_query_term\")\n",
        "\n",
        "all_engagement_with_search_query_term_derived_ordered_df = df.orderBy(['session_id', 'user_id', 'ts'], ascending=False)"
      ],
      "metadata": {
        "id": "0ykESDnTidaR"
      },
      "id": "0ykESDnTidaR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all_engagement_with_search_query_term_derived_ordered_df.filter(col(\"user_id\") == \"300452713876600234\").show()"
      ],
      "metadata": {
        "id": "gD9TzAlUt8sy"
      },
      "id": "gD9TzAlUt8sy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # GCS bucket path\n",
        "# gcs_path = \"gs://chanderiyer/projects/honeysearchbenchmarks/datasets/user_engagement/sdata_with_search_query_derived\"\n",
        "\n",
        "# # Loop through each partition and coalesce into a single file for each date subfolder\n",
        "# for date in all_engagement_with_search_query_term_derived_ordered_df.select(\"date\").distinct().collect():\n",
        "#     single_date = date[\"date\"]\n",
        "#     # Filter by the specific date\n",
        "#     filtered_df = all_engagement_with_search_query_term_derived_ordered_df.filter(F.col(\"date\") == single_date)\n",
        "#     # Coalesce the DataFrame to ensure a single file per date subfolder\n",
        "#     filtered_df.coalesce(1).write \\\n",
        "#         .option(\"header\", \"true\") \\\n",
        "#         .option(\"sep\", \"\\t\") \\\n",
        "#         .mode(\"overwrite\") \\\n",
        "#         .csv(f\"{gcs_path}/date={single_date}\")\n",
        "\n",
        "# print(f\"Data successfully written to {gcs_path} with coalesced files in each date subfolder.\")"
      ],
      "metadata": {
        "id": "KdCi_cp2pS4p"
      },
      "id": "KdCi_cp2pS4p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_engagement_with_search_query_term_derived_ordered_df = all_engagement_with_search_query_term_derived_ordered_df.filter(col(\"search_query_term\").isNotNull() & (col(\"page_title\").isNotNull()))\n",
        "all_engagement_refined_ordered_df = all_engagement_with_search_query_term_derived_ordered_df.filter(F.length(F.trim(F.col(\"search_query_term\"))) >= 2)"
      ],
      "metadata": {
        "id": "eNq4QEK8S9TT"
      },
      "id": "eNq4QEK8S9TT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "id": "xO0TANeNZEij"
      },
      "id": "xO0TANeNZEij",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession, Window\n",
        "from pyspark.sql.functions import col, lag, sum as spark_sum, when, coalesce, lit, row_number, collect_list, broadcast\n",
        "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, FloatType\n",
        "import pandas as pd\n",
        "from typing import Iterator, Tuple\n",
        "import numpy as np\n",
        "\n",
        "# ===== STEP 1: Extract and Deduplicate Search Queries & Page Titles =====\n",
        "\n",
        "# Extract unique search queries\n",
        "unique_search_queries = all_engagement_refined_ordered_df.filter(col(\"search_query_term\").isNotNull()) \\\n",
        "    .select(\"search_query_term\") \\\n",
        "    .distinct() \\\n",
        "    .repartition(200)\n",
        "\n",
        "# Extract unique page titles\n",
        "unique_page_titles = all_engagement_refined_ordered_df.filter(col(\"page_title\").isNotNull()) \\\n",
        "    .select(\"page_title\") \\\n",
        "    .distinct() \\\n",
        "    .repartition(200)\n",
        "\n",
        "# ===== STEP 2: Compute Embeddings with GPU =====\n",
        "\n",
        "# Define schema for embeddings\n",
        "embedding_schema = StructType([\n",
        "    StructField(\"text\", StringType(), False),\n",
        "    StructField(\"embedding\", ArrayType(FloatType()), False)\n",
        "])\n",
        "\n",
        "def compute_embeddings_gpu(iterator: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    import torch\n",
        "    import pandas as pd\n",
        "    import gc\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
        "\n",
        "    # Enable memory optimization\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    for pdf in iterator:\n",
        "        if len(pdf) == 0:\n",
        "            yield pd.DataFrame(columns=['text', 'embedding'])\n",
        "            continue\n",
        "\n",
        "        texts = pdf['text'].tolist()\n",
        "\n",
        "        # ⬅️ REDUCE batch size significantly\n",
        "        embeddings = model.encode(\n",
        "            texts,\n",
        "            batch_size=64,  # Changed from 512 to 64\n",
        "            show_progress_bar=False,\n",
        "            convert_to_numpy=True,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        result = pd.DataFrame({\n",
        "            'text': texts,\n",
        "            'embedding': [emb.tolist() for emb in embeddings]\n",
        "        })\n",
        "\n",
        "        # Clear GPU cache after processing\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        yield result\n",
        "\n",
        "# Compute search query embeddings\n",
        "search_query_embeddings = unique_search_queries \\\n",
        "    .withColumnRenamed(\"search_query_term\", \"text\") \\\n",
        "    .repartition(4) \\\n",
        "    .mapInPandas(compute_embeddings_gpu, schema=embedding_schema) \\\n",
        "    .withColumnRenamed(\"text\", \"search_query_term\") \\\n",
        "    .withColumnRenamed(\"embedding\", \"search_query_embedding\")\n",
        "\n",
        "# print(f\"Search query embeddings computed: {search_query_embeddings.count()}\")\n",
        "\n",
        "# Compute page title embeddings\n",
        "page_title_embeddings = unique_page_titles \\\n",
        "    .withColumnRenamed(\"page_title\", \"text\") \\\n",
        "    .repartition(128) \\\n",
        "    .mapInPandas(compute_embeddings_gpu, schema=embedding_schema) \\\n",
        "    .withColumnRenamed(\"text\", \"page_title\") \\\n",
        "    .withColumnRenamed(\"embedding\", \"page_title_embedding\")\n",
        "\n",
        "# print(f\"Page title embeddings computed: {page_title_embeddings.count()}\")\n",
        "\n",
        "# ===== STEP 3: Join Embeddings to Main DataFrame =====\n",
        "\n",
        "# Repartition main dataframe for efficient joins\n",
        "df_repartitioned = all_engagement_refined_ordered_df.repartition(400, \"search_query_term\", \"page_title\")\n",
        "\n",
        "# Broadcast smaller embedding tables for join optimization\n",
        "df_with_embeddings = df_repartitioned \\\n",
        "    .join(search_query_embeddings, on=\"search_query_term\", how=\"left\") \\\n",
        "    .join(page_title_embeddings, on=\"page_title\", how=\"left\")\n",
        "\n",
        "# ===== STEP 4: Compute Cosine Similarity with GPU =====\n",
        "\n",
        "# GPU-Accelerated Iterator-based Pandas UDF for cosine similarity\n",
        "@pandas_udf(FloatType(), functionType=PandasUDFType.SCALAR_ITER)\n",
        "def cosine_similarity_gpu(iterator: Iterator[Tuple[pd.Series, pd.Series]]) -> Iterator[pd.Series]:\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "\n",
        "    for emb1_series, emb2_series in iterator:\n",
        "        # Handle nulls\n",
        "        mask = emb1_series.notna() & emb2_series.notna()\n",
        "\n",
        "        # Initialize result\n",
        "        result = pd.Series([None] * len(emb1_series), dtype=float)\n",
        "\n",
        "        if mask.sum() == 0:\n",
        "            yield result\n",
        "            continue\n",
        "\n",
        "        # Convert to numpy arrays\n",
        "        emb1_valid = np.array(emb1_series[mask].tolist())\n",
        "        emb2_valid = np.array(emb2_series[mask].tolist())\n",
        "\n",
        "        # Compute cosine similarity in batch (vectorized)\n",
        "        similarities = (emb1_valid * emb2_valid).sum(axis=1) / (\n",
        "            np.linalg.norm(emb1_valid, axis=1) * np.linalg.norm(emb2_valid, axis=1)\n",
        "        )\n",
        "\n",
        "        # Assign back to result\n",
        "        result[mask] = similarities\n",
        "        yield result\n",
        "\n",
        "# Compute similarity scores\n",
        "df_with_similarity = df_with_embeddings.withColumn(\n",
        "    \"similarity_score\",\n",
        "    cosine_similarity_gpu(col(\"search_query_embedding\"), col(\"page_title_embedding\"))\n",
        ")\n",
        "\n",
        "# Drop embedding columns to save memory\n",
        "all_engagement_refined_ordered_final_df_with_similarity = df_with_similarity.drop(\"search_query_embedding\", \"page_title_embedding\")\n",
        "\n",
        "# print(f\"Final dataframe with similarity scores: {all_engagement_refined_ordered_final_df_with_similarity.count()}\")\n"
      ],
      "metadata": {
        "id": "cPqOakApWwgC"
      },
      "id": "cPqOakApWwgC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_engagement_refined_ordered_final_df_with_similarity = all_engagement_refined_ordered_final_df_with_similarity.orderBy(['session_id', 'user_id', 'ts'], ascending=False)\n",
        "# all_engagement_refined_ordered_final_df_with_similarity.show(200, truncate=False)"
      ],
      "metadata": {
        "id": "sEAMXUDKllJf"
      },
      "id": "sEAMXUDKllJf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_engagement_refined_ordered_final_df_with_similarity_cached = all_engagement_refined_ordered_final_df_with_similarity.cache()"
      ],
      "metadata": {
        "id": "J2-BCYeJOdQm"
      },
      "id": "J2-BCYeJOdQm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total records after refining with computed similarity: {all_engagement_refined_ordered_final_df_with_similarity_cached.count()}\")"
      ],
      "metadata": {
        "id": "9lGEEhaEOjBs"
      },
      "id": "9lGEEhaEOjBs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GCS bucket path\n",
        "gcs_path = \"gs://chanderiyer/projects/honeysearchbenchmarks/datasets/user_engagement/sdata_with_search_query_refined_coalesced\"\n",
        "\n",
        "# Loop through each partition and coalesce into a single file for each date subfolder\n",
        "for date in all_engagement_refined_ordered_final_df_with_similarity_cached.select(\"date\").distinct().collect():\n",
        "    single_date = date[\"date\"]\n",
        "    # Filter by the specific date\n",
        "    filtered_df = all_engagement_refined_ordered_final_df_with_similarity_cached.filter(F.col(\"date\") == single_date)\n",
        "    # Coalesce the DataFrame to ensure a single file per date subfolder\n",
        "    filtered_df.coalesce(1).write \\\n",
        "        .option(\"header\", \"true\") \\\n",
        "        .option(\"sep\", \"\\t\") \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .csv(f\"{gcs_path}/date={single_date}\")\n",
        "\n",
        "print(f\"Data successfully written to {gcs_path} with coalesced files in each date subfolder.\")"
      ],
      "metadata": {
        "id": "Jbri6KOzJV-7"
      },
      "id": "Jbri6KOzJV-7",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "generate_user_engagement_from_sdata_workflow.ipynb",
      "gpuType": "H100 80GB"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}